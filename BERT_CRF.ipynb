{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_CRF.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/archit/BERT_CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiOkhj3jXz93"
      },
      "source": [
        "#Library installs\n",
        "\n",
        "!pip install transformers==2.6.0\n",
        "!pip install seqeval\n",
        "!pip install urllib3 --upgrade\n",
        "!pip install pytorch-crf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Ao1TDIcvKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b97029-5b3a-4c46-8dfc-173daba7cbc7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torchtext import data\n",
        "from torchcrf import CRF\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, AdamW, BertModel\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukoab9Ojc5bn"
      },
      "source": [
        "#Load data\n",
        "train_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/train.pkl'\n",
        "val_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/val.pkl'\n",
        "\n",
        "with open(train_path, 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "  val_data = pickle.load(f)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzZUgWRHdLeO"
      },
      "source": [
        "X_train = train_data['token_final']\n",
        "X_val = val_data['token_final']\n",
        "Y_train = train_data['target_final']\n",
        "Y_val = val_data['target_final']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQD5g3Kfc9eg"
      },
      "source": [
        "MAX_LEN = 500\n",
        "BATCH_SIZE = 8\n",
        "CLASSES = {'0':0, '1':1, '[PAD]':2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIttfNLJdA_l"
      },
      "source": [
        "bert_model = 'bert-base-cased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ALnsOc5dJWy"
      },
      "source": [
        "def tokenize_bert(x, y):\n",
        "  sentence = []\n",
        "  labels = [0]\n",
        "  for word, label in zip(x, y):\n",
        "    tokenized_word = tokenizer.tokenize(word)\n",
        "    sentence.extend(tokenized_word)\n",
        "    labels.extend([label for i in range(len(tokenized_word))])\n",
        "  labels.append(0)\n",
        "  return(sentence, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ake3qFvIdSnr"
      },
      "source": [
        "#Tokenize the data using bert tokenizer which is based on WordPiece tokenization\n",
        "len_train = len(X_train)\n",
        "len_val = len(X_val)\n",
        "\n",
        "for i in range(len_train):\n",
        "  X_train[i], Y_train[i] = tokenize_bert(X_train[i], Y_train[i])\n",
        "\n",
        "for i in range(len_val):\n",
        "  X_val[i], Y_val[i] = tokenize_bert(X_val[i], Y_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGQLOG-zdodO"
      },
      "source": [
        "#Pad the input data so that we can deal with them as tensors\n",
        "X_train_id = pad_sequences([tokenizer.encode(text) for text in X_train], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_train_id = pad_sequences(Y_train, maxlen=MAX_LEN, value=0, dtype='long', truncating='post', padding='post')\n",
        "X_val_id = pad_sequences([tokenizer.encode(text) for text in X_val], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_val_id = pad_sequences(Y_val, maxlen=MAX_LEN, value=0, dtype='long', truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyJqsNTsd0Mu"
      },
      "source": [
        "def get_attention_mask(x):\n",
        "  return([[(i!=0) for i in text] for text in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6TxNDkNd22r"
      },
      "source": [
        "#Generating masks\n",
        "attention_mask_train = get_attention_mask(X_train_id)\n",
        "attention_mask_val = get_attention_mask(X_val_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGd6PqfWd5p1"
      },
      "source": [
        "X_train_id = torch.tensor(X_train_id)\n",
        "Y_train_id = torch.tensor(Y_train_id)\n",
        "X_val_id = torch.tensor(X_val_id)\n",
        "Y_val_id = torch.tensor(Y_val_id)\n",
        "attention_mask_train = torch.tensor(attention_mask_train)\n",
        "attention_mask_val = torch.tensor(attention_mask_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4qcgg_5d6bG"
      },
      "source": [
        "#Creating dataloaders for train and val data, this will allow us to easily get batches during training\n",
        "\n",
        "train_data = TensorDataset(X_train_id, attention_mask_train, Y_train_id)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(X_val_id, attention_mask_val, Y_val_id)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKna_2y7eFWd"
      },
      "source": [
        "#Defining model for BERT-CRF and BERT-BiLSTM-CRF\n",
        "#Parameters to class : \n",
        "#     bert_model : Name of pretrained bert model to use\n",
        "#     num_labels : No. of classes\n",
        "#     bilstm : Whether to include the bilstm layer after bert model\n",
        "\n",
        "class BertCRFModel(nn.Module):\n",
        "  def __init__(self, bert_model, num_labels, bilstm):\n",
        "    super(BertCRFModel, self).__init__()\n",
        "    self.bert_model = bert_model\n",
        "    self.num_labels = num_labels\n",
        "    self.bert = BertModel.from_pretrained(self.bert_model, output_attentions=False, output_hidden_states=False)\n",
        "    self.crf = CRF(self.num_labels, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.7)\n",
        "    if(bilstm):\n",
        "      self.bilstm = nn.LSTM(self.bert.config.hidden_size, self.num_labels, bidirectional=True, num_layers=1, batch_first=True)\n",
        "      self.fc = nn.Linear(self.num_labels*2, self.num_labels)\n",
        "    else:\n",
        "      self.fc = nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
        "    \n",
        "  def forward(self, inputs, masks, labels=None, bilstm=False):\n",
        "    outputs = self.bert(inputs, masks)\n",
        "    seq_out = outputs[0]\n",
        "    \n",
        "    if(not bilstm):\n",
        "      x = self.fc(seq_out)\n",
        "      seq_out = self.dropout(seq_out)\n",
        "    else:\n",
        "      seq_out = nn.utils.rnn.pack_padded_sequence(seq_out, torch.tensor([torch.sum(a) for a in masks]), batch_first=True, enforce_sorted=False)\n",
        "      x, (h_n, c_n) = self.bilstm(seq_out)\n",
        "      x, _ = nn.utils.rnn.pad_packed_sequence(x, total_length=500, batch_first=True)\n",
        "      x = self.fc(x)\n",
        "\n",
        "    masks = masks.type(torch.uint8)\n",
        "    if(labels is not None):\n",
        "      loss = -self.crf(F.log_softmax(x, dim=2), labels, mask=masks, reduction='mean')\n",
        "      preds = self.crf.decode(x, mask=masks)\n",
        "      return loss, preds\n",
        "    else:\n",
        "      preds = self.crf.decode(x, mask=masks)\n",
        "      return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iberlFePyep"
      },
      "source": [
        "NUM_LABELS = 2\n",
        "model = BertCRFModel(bert_model, NUM_LABELS, False)\n",
        "# model = torch.load('/content/drive/My Drive/model5.pt')\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWMBYMdBQJw-"
      },
      "source": [
        "#Initialize AdamW optimizer with weight decay for regularization\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "                                {'params' : [p for n,p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate' : 0.01},\n",
        "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
        "\n",
        "optimizer_grouped_parameters_no_bert = [\n",
        "                                {'params' : [p for n,p in param_optimizer if 'bert' not in n], 'weight_decay_rate' : 0.01},\n",
        "                                ]\n",
        "optimizer_initial = AdamW(optimizer_grouped_parameters_no_bert, lr=3e-5, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTHh71PyRFHo"
      },
      "source": [
        "#Initialize scheduler to perform learning rate decay\n",
        "epochs = 10\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer_initial,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "scheduler_2 = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn1TtCr7laE9"
      },
      "source": [
        "l = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZouBEdzQd8GQ"
      },
      "source": [
        "#Code for training model and evaluating on validaition data\n",
        "\n",
        "train_loss, val_loss = [], []\n",
        "train_acc, val_acc = [], []\n",
        "train_f1, val_f1 = [], []\n",
        "\n",
        "for epoch in trange(epochs, desc = 'Epoch'):\n",
        "  model.train()\n",
        "  t_loss, t_acc = 0, 0\n",
        "  predictions, true_labels = [], []\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    model.zero_grad()\n",
        "    loss, preds = model(b_input_id, b_input_mask, b_labels, False)\n",
        "    loss.backward()\n",
        "    t_loss += loss.item()\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "    \n",
        "    #Used to perform the first round of training in which the bert model is frozen the rest of the model is trained to convergence\n",
        "    # optimizer_initial.step()\n",
        "    # scheduler.step()\n",
        "     \n",
        "    #Used to perform the second round of training in which the bert model is unfrozen and whole model is finetuned\n",
        "    optimizer.step()\n",
        "    scheduler_2.step()\n",
        "    \n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    predictions.extend([p for p in preds])\n",
        "    true_labels.extend([l_i.item() for l,x in zip(b_labels, b_input_id) for l_i,x_i in zip(l,x) if x_i!=0])\n",
        "    \n",
        "  print(f\"Train Loss : {t_loss/len(train_dataloader)}\")\n",
        "  train_loss.append(t_loss/len(train_dataloader))\n",
        "  pred_tags = [p_i for p in predictions for p_i in p]\n",
        "  valid_tags = true_labels\n",
        "  # valid_tags = [l_i for l in true_labels\n",
        "  #                               for l_i in l if l_i != 2]\n",
        "  train_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  train_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print(\"Train Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Train F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  print()\n",
        "\n",
        "  #Evaluation on val data\n",
        "  model.eval()\n",
        "  v_loss, v_accuracy = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "  for batch in val_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "      loss, preds = model(b_input_id, b_input_mask, b_labels, False)\n",
        "      \n",
        "    v_loss += loss.item()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    predictions.extend([p for p in preds])\n",
        "    true_labels.extend([l_i.item() for l,x in zip(b_labels, b_input_id) for l_i,x_i in zip(l,x) if x_i!=0])\n",
        "    \n",
        "  v_loss = v_loss/len(val_dataloader)\n",
        "  val_loss.append(v_loss)\n",
        "  if(v_loss < l):\n",
        "    l = v_loss\n",
        "    print(\"Model Checkpoint\")\n",
        "  torch.save(model, f'/content/drive/My Drive/model{epoch}.pt')\n",
        "    \n",
        "  print(f\"Validation Loss : {v_loss}\")\n",
        "  pred_tags = [p_i for p in predictions for p_i in p]\n",
        "  valid_tags = true_labels\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  val_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  val_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Span_Avg_F1_Metric.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YDiPjZwGezY47BVKkGu4uapbxiZfxZuG",
      "authorship_tag": "ABX9TyMrHH2xxmf3HQmHRLtb/rpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "864c414af86d461cba90c6760871580a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e42b55bde4d449fa3e7f114dd5b5533",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_572a4bae236c4cb6a737d03c6d242e17",
              "IPY_MODEL_e97442204c994c88b10f1aeb31ee5e68"
            ]
          }
        },
        "6e42b55bde4d449fa3e7f114dd5b5533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "572a4bae236c4cb6a737d03c6d242e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c5400dfa5a164c229b4f8a546f8927a9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38d9d058d91d40359d9f5c64ae46bcf1"
          }
        },
        "e97442204c994c88b10f1aeb31ee5e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd34946094754034ac497397dbc4a60d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 2.68MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b20ec2a857b14a0ebf27cbbc02655462"
          }
        },
        "c5400dfa5a164c229b4f8a546f8927a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38d9d058d91d40359d9f5c64ae46bcf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd34946094754034ac497397dbc4a60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b20ec2a857b14a0ebf27cbbc02655462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/archit/Span_Avg_F1_Metric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4NVHTVxBoPC",
        "outputId": "02e19f01-4273-42bf-b51f-cccd12d72edf"
      },
      "source": [
        "!pip install transformers==2.6.0\n",
        "!pip install seqeval\n",
        "!pip install urllib3 --upgrade\n",
        "!pip install pytorch-crf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 12.1MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/f5/aeb4d65266f7712a627674bd19994cee3e1c66ff588adbc4db3fc0bbbf97/boto3-1.16.34-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 38.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.5)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.3MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.34\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/ef/e35e41d6e445f472ac8f4fca8dd22726d8c6dc19ab06317164a222d13599/botocore-1.19.34-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.17.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.34->boto3->transformers==2.6.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=35645ecf25bfa6d3db9b13577d808bee419c32251995622bc0508e5226e21ba6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.34 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed boto3-1.16.34 botocore-1.19.34 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.5.2 transformers-2.6.0\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=084746556281de1b6cc2ea85c397fd1108bb12cf9ece64a9c08274f3caa50c04\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting urllib3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 14.9MB/s \n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed urllib3-1.26.2\n",
            "Collecting pytorch-crf\n",
            "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7GkJI_zBuY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f20917-33d4-4be7-c520-46596ea9e4bd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torchtext import data\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, AdamW, BertModel\n",
        "\n",
        "from torchcrf import CRF\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "torch.manual_seed(12)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URjZyFkzuss_",
        "outputId": "feeb0fbc-0813-4334-ce81-4efc3ffe6b4c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBKM68AoByq6"
      },
      "source": [
        "#Load data\n",
        "train_path = '/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/train.pkl'\n",
        "val_path = '/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/val.pkl'\n",
        "\n",
        "# train_path = '/content/drive/MyDrive/ToxicSpan_CS669V/processed/finaltrain.pkl'\n",
        "# val_path = '/content/drive/MyDrive/ToxicSpan_CS669V/processed/finaldev.pkl'\n",
        "\n",
        "with open(train_path, 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "  val_data = pickle.load(f)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ez-gi7AB4I6"
      },
      "source": [
        "#Class to take in tokenizer and model along with input data and calculate the span average f1 score\n",
        "class SpanAvgF1():\n",
        "  def __init__(self, model, tokenizer, X, y, spans, target_spans, maxlen=500):\n",
        "    self.model = model\n",
        "    self.model.cuda()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.maxlen = maxlen\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.spans = spans\n",
        "    self.target_spans = target_spans\n",
        "    self.prepare_data()\n",
        "\n",
        "    self.X = torch.tensor(self.X)\n",
        "    self.y = torch.tensor(self.y)\n",
        "    self.attention_mask = torch.tensor(self.attention_mask)\n",
        "\n",
        "    data = TensorDataset(self.X, self.attention_mask, self.y)\n",
        "    self.dataloader = DataLoader(data, batch_size=16, shuffle=False)\n",
        "  \n",
        "  def tokenize_data(self, x, y, s):\n",
        "    sentence = []\n",
        "    labels = [0]\n",
        "    spans = []\n",
        "    for i in range(len(x)):\n",
        "      word = x[i]\n",
        "      label = y[i]\n",
        "      tokenized_word = self.tokenizer.tokenize(word)\n",
        "      sentence.extend(tokenized_word)\n",
        "      labels.extend([label for k in range(len(tokenized_word))])\n",
        "      curr = s[i][0]\n",
        "      spans.append([curr, curr+len(tokenized_word[0])])\n",
        "      curr += len(tokenized_word[0])\n",
        "      for j in range(len(tokenized_word)-1):\n",
        "        spans.append([curr, curr+len(tokenized_word[j+1])-2])\n",
        "        curr += len(tokenized_word[j+1])-2\n",
        "      spans[-1][-1] = s[i][1]\n",
        "    labels.append(0)\n",
        "    return(sentence, labels, spans)\n",
        "\n",
        "  def get_attention_mask(self, x):\n",
        "    return([[(i!=0) for i in text] for text in x])\n",
        "\n",
        "  def prepare_data(self):\n",
        "    for i in range(len(self.X)):\n",
        "      self.X[i], self.y[i], self.spans[i] = self.tokenize_data(self.X[i], self.y[i], self.spans[i])\n",
        "    self.X = pad_sequences([tokenizer.encode(text) for text in self.X], maxlen = self.maxlen, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "    self.y = pad_sequences(self.y, maxlen=self.maxlen, value=2, dtype='long', truncating='post', padding='post')\n",
        "    self.attention_mask = self.get_attention_mask(self.X)\n",
        "  \n",
        "  def get_text_lengths(self, masks):\n",
        "    lengths = []\n",
        "    for mask in masks:\n",
        "      lengths.append(torch.sum(mask).item())\n",
        "    return(lengths)\n",
        "\n",
        "  def evaluate(self, flag):\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in self.dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_id, b_input_mask, b_labels = batch\n",
        "      self.model.eval()\n",
        "      \n",
        "      if(flag==1):\n",
        "        with torch.no_grad():\n",
        "          outputs = self.model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "      \n",
        "      elif(flag==2):\n",
        "        with torch.no_grad():\n",
        "          out = model(b_input_id, b_input_mask)\n",
        "          logits = torch.argmax(F.softmax(out, dim=2), dim=2).to('cpu').numpy()\n",
        "          predictions.extend(logits)\n",
        "      \n",
        "      else:\n",
        "        preds = model(b_input_id, b_input_mask, None, True)\n",
        "        predictions.extend([p for p in preds])\n",
        "\n",
        "      true_labels.extend(b_labels)\n",
        "    if(flag!=3):\n",
        "      self.pred_tags = [[p_i for p_i, l_i in zip(p, l) if l_i != 2][1:-1] for p, l in zip(predictions, true_labels)]\n",
        "    else:\n",
        "      self.pred_tags = predictions\n",
        "\n",
        "  def f1score(self):\n",
        "    self.f1list = []\n",
        "    self.predicted_spans = []\n",
        "    f1 = 0\n",
        "    for i in range(len(self.spans)):\n",
        "      s = [0 for k in range(len(val_data['text'][i]))]\n",
        "      prev = 0\n",
        "      for j in range(len(self.spans[i])):\n",
        "        for k in range(self.spans[i][j][0], self.spans[i][j][1]):\n",
        "          s[k] = self.pred_tags[i][j]\n",
        "        if(prev==1 and self.pred_tags[i][j]==1):\n",
        "          for l in range(self.spans[i][j-1][1], self.spans[i][j][0]):\n",
        "            s[l] = 1\n",
        "        \n",
        "        prev = self.pred_tags[i][j]\n",
        "      self.f1list.append(f1_score(self.target_spans[i], s))\n",
        "      f1 += f1_score(self.target_spans[i], s, zero_division=1)\n",
        "      self.predicted_spans.append(s)\n",
        "    return(f1/len(self.X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYSvVi-qmUEZ"
      },
      "source": [
        "X = val_data['token_final'].values\n",
        "Y = val_data['target_final'].values\n",
        "spans = val_data['span_final'].values\n",
        "target_spans = []\n",
        "for i in range(len(X)):\n",
        "  s = [0 for j in range(len(val_data['text'][i]))]\n",
        "  for k in val_data['spans'][i]:\n",
        "    s[k] = 1\n",
        "  target_spans.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az7BE_0-PrsZ"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, bert, input_dim, num_labels, hidden_dim, lstm_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.num_labels = num_labels\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.bert = bert\n",
        "    self.bilstm = nn.LSTM(input_dim, hidden_dim, lstm_layers, bidirectional=True, dropout=dropout, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim*2, num_labels)\n",
        "\n",
        "  def forward(self, inputs, masks, labels=None, bilstm=False):\n",
        "    outputs = self.bert(inputs, masks)\n",
        "    seq_out = outputs[0]\n",
        "    \n",
        "    seq_out = nn.utils.rnn.pack_padded_sequence(seq_out, torch.tensor([torch.sum(a) for a in masks]), batch_first=True, enforce_sorted=False)\n",
        "    x, (h_n, c_n) = self.bilstm(seq_out)\n",
        "    x, _ = nn.utils.rnn.pad_packed_sequence(x, total_length=300, batch_first=True)\n",
        "    x = self.fc(x)\n",
        "    return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMZr6vQdPSCu"
      },
      "source": [
        "class BertCRFModel(nn.Module):\n",
        "  def __init__(self, bert_model, num_labels, bilstm):\n",
        "    super(BertCRFModel, self).__init__()\n",
        "    self.bert_model = bert_model\n",
        "    self.num_labels = num_labels\n",
        "    self.bert = BertModel.from_pretrained(self.bert_model, output_attentions=False, output_hidden_states=False)\n",
        "    self.crf = CRF(self.num_labels, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.7)\n",
        "    if(bilstm):\n",
        "      self.bilstm = nn.LSTM(self.bert.config.hidden_size, self.num_labels, bidirectional=True, num_layers=1, batch_first=True)\n",
        "      self.fc = nn.Linear(self.num_labels*2, self.num_labels)\n",
        "    else:\n",
        "      self.fc = nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
        "    \n",
        "  def forward(self, inputs, masks, labels=None, bilstm=False):\n",
        "    outputs = self.bert(inputs, masks)\n",
        "    seq_out = outputs[0]\n",
        "    \n",
        "    if(not bilstm):\n",
        "      x = self.fc(seq_out)\n",
        "      seq_out = self.dropout(seq_out)\n",
        "    else:\n",
        "      seq_out = nn.utils.rnn.pack_padded_sequence(seq_out, torch.tensor([torch.sum(a) for a in masks]), batch_first=True, enforce_sorted=False)\n",
        "      x, (h_n, c_n) = self.bilstm(seq_out)\n",
        "      x, _ = nn.utils.rnn.pad_packed_sequence(x, total_length=500, batch_first=True)\n",
        "      x = self.fc(x)\n",
        "\n",
        "    masks = masks.type(torch.uint8)\n",
        "    if(labels is not None):\n",
        "      loss = -self.crf(F.log_softmax(x, dim=2), labels, mask=masks, reduction='mean')\n",
        "      preds = self.crf.decode(x, mask=masks)\n",
        "      return loss, preds\n",
        "    else:\n",
        "      preds = self.crf.decode(x, mask=masks)\n",
        "      return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uE-mJ6womcZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "864c414af86d461cba90c6760871580a",
            "6e42b55bde4d449fa3e7f114dd5b5533",
            "572a4bae236c4cb6a737d03c6d242e17",
            "e97442204c994c88b10f1aeb31ee5e68",
            "c5400dfa5a164c229b4f8a546f8927a9",
            "38d9d058d91d40359d9f5c64ae46bcf1",
            "bd34946094754034ac497397dbc4a60d",
            "b20ec2a857b14a0ebf27cbbc02655462"
          ]
        },
        "outputId": "09d4643e-06fc-4f10-85bf-a26199dd9e32"
      },
      "source": [
        "model = torch.load('/content/drive/MyDrive/semi_sup-5.pt', map_location=torch.device('cpu'))\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "864c414af86d461cba90c6760871580a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26PABPKZApYq",
        "outputId": "e84dd880-d20b-4b5d-962d-72e45a8787ca"
      },
      "source": [
        "metric = SpanAvgF1(model, tokenizer, X, Y, spans, target_spans, 500)\n",
        "metric.evaluate(1)\n",
        "metric.f1score()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6882755826103494"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU3PbJhdQNIK",
        "outputId": "d84e24d2-98ed-4b74-b1fd-0ed7a5044a2f"
      },
      "source": [
        "metric.pred_tags[4], val_data['target_final'][4], val_data['token_final'][4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " ['Good',\n",
              "  'points',\n",
              "  'A',\n",
              "  'dumb',\n",
              "  'crude',\n",
              "  'guy',\n",
              "  'in',\n",
              "  'a',\n",
              "  'dumb',\n",
              "  'crude',\n",
              "  'sport',\n",
              "  'That',\n",
              "  \"'\",\n",
              "  's',\n",
              "  'what',\n",
              "  'Trump',\n",
              "  'has',\n",
              "  'done',\n",
              "  'to',\n",
              "  'politics',\n",
              "  'I'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPut_61pb83d"
      },
      "source": [
        "df = pd.DataFrame(list(zip(val_data['token_final'], metric.pred_tags)), \n",
        "               columns =['token_final', 'target_final'])\n",
        "df.to_csv('/content/drive/My Drive/ToxicSpan_CS669V/processed/val_semi_new.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJwUrW9AKfXJ"
      },
      "source": [
        "#Generating csv file to compare ground truth and predicted spans\n",
        "text = []\n",
        "truth_text = []\n",
        "pred_text = []\n",
        "\n",
        "for i in range(len(val_data['text'])):\n",
        "  text.append(val_data['text'][i])\n",
        "  arr = []\n",
        "  for j,k in enumerate(val_data['target_final'][i][1:-1]):\n",
        "    if(k == 1):\n",
        "      arr.append(val_data['token_final'][i][j])\n",
        "  truth_text.append(arr)\n",
        "  arr = []\n",
        "  # x = tokenizer.convert_ids_to_tokens(metric.X[i])\n",
        "  for j,k in enumerate(metric.pred_tags[i]):\n",
        "    if(k == 1):\n",
        "      arr.append(val_data['token_final'][i][j])\n",
        "  pred_text.append(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShaNB_D_Md4C"
      },
      "source": [
        "df = pd.DataFrame(list(zip(text, truth_text, pred_text)), \n",
        "               columns =['Text', 'Ground Truth', 'Predicted'])\n",
        "df.to_csv('/content/drive/My Drive/ToxicSpan_CS669V/processed/semi_sup_it3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
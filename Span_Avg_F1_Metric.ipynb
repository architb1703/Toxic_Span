{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Span_Avg_F1_Metric.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YDiPjZwGezY47BVKkGu4uapbxiZfxZuG",
      "authorship_tag": "ABX9TyMLKZkXRe0UODoFxcVrbtlk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/archit/Span_Avg_F1_Metric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4NVHTVxBoPC",
        "outputId": "90ba4bb1-737e-4430-c027-7df076902620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers==2.6.0\n",
        "!pip install seqeval\n",
        "!pip install urllib3 --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 18.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 13.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 13.4MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 11.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 10.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 18.6MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/83/50351bf647a09d54320665233258ee1a22f3a4ce878a51ab2cfb69186af7/boto3-1.16.19-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 39.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Collecting botocore<1.20.0,>=1.19.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/9e/a51bb1c8338751b542031e3c2a3c8853005d10c803e0685ad0094d5bd0b3/botocore-1.19.19-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 40.1MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.17.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.19->boto3->transformers==2.6.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=0895755fcc5246ee2905dbf3a330f6b4cdb90c1c13b1863932fc135405c2b189\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.19 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, transformers\n",
            "Successfully installed boto3-1.16.19 botocore-1.19.19 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.5.2 transformers-2.6.0\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=d60b7bb0f20b63a1de6608d2b834e85e680f72e185618bca24193930565e2cfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting urllib3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 10.8MB/s \n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed urllib3-1.26.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7GkJI_zBuY6",
        "outputId": "d33be8df-2d7a-4eaa-e8a6-052b2cae27dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torchtext import data\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBKM68AoByq6"
      },
      "source": [
        "#Load data\n",
        "train_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/train.pkl'\n",
        "val_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/val.pkl'\n",
        "\n",
        "with open(train_path, 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "  val_data = pickle.load(f)\n",
        "  f.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ez-gi7AB4I6"
      },
      "source": [
        "#Class to take in tokenizer and model along with input data and calculate the span average f1 score\n",
        "class SpanAvgF1():\n",
        "  def __init__(self, model, tokenizer, X, y, spans, target_spans):\n",
        "    self.model = model\n",
        "    self.model.cuda()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.spans = spans\n",
        "    self.target_spans = target_spans\n",
        "    self.prepare_data()\n",
        "\n",
        "    self.X = torch.tensor(self.X)\n",
        "    self.y = torch.tensor(self.y)\n",
        "    self.attention_mask = torch.tensor(self.attention_mask)\n",
        "\n",
        "    data = TensorDataset(self.X, self.attention_mask, self.y)\n",
        "    self.dataloader = DataLoader(data, batch_size=16, shuffle=False)\n",
        "\n",
        "    self.evaluate()\n",
        "  \n",
        "  def tokenize_data(self, x, y, s):\n",
        "    sentence = []\n",
        "    labels = []\n",
        "    spans = []\n",
        "    for i in range(len(x)):\n",
        "      word = x[i]\n",
        "      label = y[i]\n",
        "      tokenized_word = self.tokenizer.tokenize(word)\n",
        "      sentence.extend(tokenized_word)\n",
        "      labels.extend([label for k in range(len(tokenized_word))])\n",
        "      curr = s[i][0]\n",
        "      spans.append([curr, curr+len(tokenized_word[0])])\n",
        "      curr += len(tokenized_word[0])\n",
        "      for j in range(len(tokenized_word)-1):\n",
        "        spans.append([curr, curr+len(tokenized_word[j+1])-2])\n",
        "        curr += len(tokenized_word[j+1])-2\n",
        "      spans[-1][-1] = s[i][1]\n",
        "    return(sentence, labels, spans)\n",
        "\n",
        "  def get_attention_mask(self, x):\n",
        "    return([[(i!=0) for i in text] for text in x])\n",
        "\n",
        "  def prepare_data(self):\n",
        "    for i in range(len(self.X)):\n",
        "      self.X[i], self.y[i], self.spans[i] = self.tokenize_data(self.X[i], self.y[i], self.spans[i])\n",
        "    self.X = pad_sequences([tokenizer.convert_tokens_to_ids(text) for text in self.X], maxlen = 500, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "    self.y = pad_sequences(self.y, maxlen=500, value=2, dtype='long', truncating='post', padding='post')\n",
        "    self.attention_mask = self.get_attention_mask(self.X)\n",
        "  \n",
        "  def evaluate(self):\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in self.dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_id, b_input_mask, b_labels = batch\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        outputs = self.model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      \n",
        "      logits = outputs[1].detach().cpu().numpy()\n",
        "      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "      true_labels.extend(b_labels)\n",
        "\n",
        "    self.pred_tags = [[p_i for p_i, l_i in zip(p, l) if l_i != 2] for p, l in zip(predictions, true_labels)]\n",
        "\n",
        "  def f1score(self):\n",
        "    self.f1list = []\n",
        "    self.predicted_spans = []\n",
        "    f1 = 0\n",
        "    for i in range(len(self.spans)):\n",
        "      s = [0 for k in range(self.spans[i][-1][1])]\n",
        "      prev = 0\n",
        "      for j in range(len(self.spans[i])):\n",
        "        for k in range(self.spans[i][j][0], self.spans[i][j][1]):\n",
        "          s[k] = self.pred_tags[i][j]\n",
        "        if(prev==1 and self.pred_tags[i][j]==1):\n",
        "          for k in range(self.spans[i][j-1][1], self.spans[i][j][0]):\n",
        "            s[k] = 1\n",
        "        prev = self.pred_tags[i][j]\n",
        "      self.f1list.append(f1_score(self.target_spans[i], s))\n",
        "      f1 += f1_score(self.target_spans[i], s, zero_division=1)\n",
        "      self.predicted_spans.append(s)\n",
        "    return(f1/len(self.X))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYSvVi-qmUEZ"
      },
      "source": [
        "X = val_data['token_final'].values\n",
        "Y = val_data['target_final'].values\n",
        "spans = val_data['span_final'].values\n",
        "target_spans = []\n",
        "for i in range(len(X)):\n",
        "  s = [0 for j in range(spans[i][-1][-1])]\n",
        "  for k in val_data['spans'][i]:\n",
        "    s[k] = 1\n",
        "  target_spans.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az7BE_0-PrsZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uE-mJ6womcZ"
      },
      "source": [
        "model = torch.load('/content/drive/My Drive/bert_base_cased_best.pt', map_location=torch.device('cpu'))\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw7NeLMKoeDU",
        "outputId": "46a9be89-bae4-4675-9c12-ab77638fe7a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "metric = SpanAvgF1(model, tokenizer, X, Y, spans, target_spans)\n",
        "metric.f1score()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6571616805131474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU3PbJhdQNIK",
        "outputId": "4d780ce4-170f-40c2-aa97-38c494ff75cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "val_data['token_final']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [Fuck, ##ing, Left, ##ist, He, ##bes, ,, alway...\n",
              "1      [Because, plants, are, D, ##AN, ##GE, ##RO, ##...\n",
              "2      [Their, is, so, much, additional, garbage, tag...\n",
              "3      [Are, there, really, enough, red, neck, idiot,...\n",
              "4      [Good, points, A, dumb, crude, guy, in, a, dum...\n",
              "                             ...                        \n",
              "789    [It, never, passes, comment, review, but, I, g...\n",
              "790    [I, was, n, ', t, there, and, support, it, %, ...\n",
              "791    [funny, how, these, churches, want, to, protec...\n",
              "792    [Typical, lying, protest, ##or, They, ex, ##ag...\n",
              "793    [He, trans, ##cend, ##ed, the, ', civil, ##iti...\n",
              "Name: token_final, Length: 794, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJwUrW9AKfXJ"
      },
      "source": [
        "#Generating csv file to compare ground truth and predicted spans\n",
        "text = []\n",
        "truth_text = []\n",
        "pred_text = []\n",
        "\n",
        "for i in range(len(val_data['text'])):\n",
        "  text.append(val_data['text'][i])\n",
        "  arr = []\n",
        "  for j,k in enumerate(val_data['target_final'][i]):\n",
        "    if(k == 1):\n",
        "      arr.append(val_data['token_final'][i][j])\n",
        "  truth_text.append(arr)\n",
        "  arr = []\n",
        "  x = tokenizer.convert_ids_to_tokens(metric.X[i])\n",
        "  for j,k in enumerate(metric.pred_tags[i]):\n",
        "    if(k == 1):\n",
        "      arr.append(x[j])\n",
        "  pred_text.append(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShaNB_D_Md4C"
      },
      "source": [
        "df = pd.DataFrame(list(zip(text, truth_text, pred_text)), \n",
        "               columns =['Text', 'Ground Truth', 'Predicted'])\n",
        "df.to_csv('/content/drive/My Drive/ToxicSpan_CS669V/processed/bert_base_cased_best_answer.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
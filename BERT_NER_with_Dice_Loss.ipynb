{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT-NER with Dice Loss",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/archit/BERT_NER_with_Dice_Loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmhCJJpF225N",
        "outputId": "d6f4220c-64aa-4f0a-c32b-94d167546725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers==2.6.0\n",
        "!pip install seqeval\n",
        "!pip install urllib3 --upgrade\n",
        "!pip install sadice"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.6.0 in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.1.94)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.16.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.19.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.18->boto3->transformers==2.6.0) (2.8.1)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.26.2\n",
            "    Uninstalling urllib3-1.26.2:\n",
            "      Successfully uninstalled urllib3-1.26.2\n",
            "Successfully installed urllib3-1.25.11\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Collecting urllib3\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.25.11\n",
            "    Uninstalling urllib3-1.25.11:\n",
            "      Successfully uninstalled urllib3-1.25.11\n",
            "Successfully installed urllib3-1.26.2\n",
            "Requirement already satisfied: sadice in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from sadice) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn4Uqbz-4-dJ",
        "outputId": "35170584-d9da-4979-b2d8-d25cfa34789e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "#Based on the Bert for NER from https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torchtext import data\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "from sadice import SelfAdjDiceLoss\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3bf7e045e5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSad5IfQr27p",
        "outputId": "2bf79b53-b7bc-4d2f-8653-8d08fb21f822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmkRlyPK5Qtq"
      },
      "source": [
        "train_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/train.pkl'\n",
        "val_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/val.pkl'\n",
        "\n",
        "with open(train_path, 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "  val_data = pickle.load(f)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WxA6XvA66Ha",
        "outputId": "e020ba3b-353b-4db3-bb7b-11a24ed51f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "      <th>target_final</th>\n",
              "      <th>span_final</th>\n",
              "      <th>token_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
              "      <td>Fucking Leftist Hebes, always finding the dirt...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[(0, 7), (8, 15), (16, 21), (21, 22), (23, 29)...</td>\n",
              "      <td>[Fucking, Leftist, Hebes, ,, always, finding, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[62, 63, 64, 65]</td>\n",
              "      <td>Because 13 plants are DANGEROUS.  SMDH.\\n\\nGro...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[(0, 7), (11, 17), (18, 21), (22, 32), (34, 39...</td>\n",
              "      <td>[Because, plants, are, DANGEROUS, SMDH, Grow, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[28, 29, 30, 31, 32, 33, 34]</td>\n",
              "      <td>Their is so much additional garbage tagged on ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 5), (6, 8), (9, 11), (12, 16), (17, 27), ...</td>\n",
              "      <td>[Their, is, so, much, additional, garbage, tag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n",
              "      <td>Are there really enough red neck idiots in Was...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 3), (4, 9), (10, 16), (17, 23), (24, 27),...</td>\n",
              "      <td>[Are, there, really, enough, red, neck, idiots...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2...</td>\n",
              "      <td>Good points.  A dumb crude guy in a dumb crude...</td>\n",
              "      <td>[0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 4), (5, 12), (14, 15), (16, 20), (21, 26)...</td>\n",
              "      <td>[Good, points, A, dumb, crude, guy, in, a, dum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>[58, 59, 60, 61]</td>\n",
              "      <td>It never passes comment review but I gotta try...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
              "      <td>[(0, 2), (3, 8), (9, 15), (16, 23), (24, 30), ...</td>\n",
              "      <td>[It, never, passes, comment, review, but, I, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>[]</td>\n",
              "      <td>I wasn't there and support it 100%.\\nYour murd...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 1), (2, 5), (5, 8), (9, 14), (15, 18), (1...</td>\n",
              "      <td>[I, was, n't, there, and, support, it, %, Your...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>[320, 321, 322, 323, 324, 325, 326, 327]</td>\n",
              "      <td>funny how these churches want to protect illeg...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 5), (6, 9), (10, 15), (16, 24), (25, 29),...</td>\n",
              "      <td>[funny, how, these, churches, want, to, protec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>[78, 79, 80, 81, 82, 83, 84, 85, 86]</td>\n",
              "      <td>Typical lying protestor. They exaggerate every...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 7), (8, 13), (14, 24), (25, 29), (30, 40)...</td>\n",
              "      <td>[Typical, lying, protestor, They, exaggerate, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>[484, 485, 486, 487, 488, 522, 523, 524, 525, ...</td>\n",
              "      <td>He transcended the \"civilities of internationa...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 2), (3, 14), (15, 18), (19, 30), (31, 33)...</td>\n",
              "      <td>[He, transcended, the, 'civilities, of, intern...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>794 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 spans  ...                                        token_final\n",
              "0                                [0, 1, 2, 3, 4, 5, 6]  ...  [Fucking, Leftist, Hebes, ,, always, finding, ...\n",
              "1                                     [62, 63, 64, 65]  ...  [Because, plants, are, DANGEROUS, SMDH, Grow, ...\n",
              "2                         [28, 29, 30, 31, 32, 33, 34]  ...  [Their, is, so, much, additional, garbage, tag...\n",
              "3    [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...  ...  [Are, there, really, enough, red, neck, idiots...\n",
              "4    [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2...  ...  [Good, points, A, dumb, crude, guy, in, a, dum...\n",
              "..                                                 ...  ...                                                ...\n",
              "789                                   [58, 59, 60, 61]  ...  [It, never, passes, comment, review, but, I, g...\n",
              "790                                                 []  ...  [I, was, n't, there, and, support, it, %, Your...\n",
              "791           [320, 321, 322, 323, 324, 325, 326, 327]  ...  [funny, how, these, churches, want, to, protec...\n",
              "792               [78, 79, 80, 81, 82, 83, 84, 85, 86]  ...  [Typical, lying, protestor, They, exaggerate, ...\n",
              "793  [484, 485, 486, 487, 488, 522, 523, 524, 525, ...  ...  [He, transcended, the, 'civilities, of, intern...\n",
              "\n",
              "[794 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-EqtoyUhcpc"
      },
      "source": [
        "MAX_LEN = 500\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DePWwE_JoDu0"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnSsn5MXoTxT"
      },
      "source": [
        "X_train = train_data['token_final']\n",
        "X_val = val_data['token_final']\n",
        "Y_train = train_data['target_final']\n",
        "Y_val = val_data['target_final']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G8KiiEUTSdY"
      },
      "source": [
        "CLASSES = {'0':0, '1':1, '[PAD]':2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsYGVo5nouK0"
      },
      "source": [
        "def tokenize_bert(x, y):\n",
        "  sentence = []\n",
        "  labels = [0]\n",
        "  for word, label in zip(x, y):\n",
        "    tokenized_word = tokenizer.tokenize(word)\n",
        "    sentence.extend(tokenized_word)\n",
        "    labels.extend([label for i in range(len(tokenized_word))])\n",
        "  labels.append(0)\n",
        "  return(sentence, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3eyhTSSpPsa"
      },
      "source": [
        "len_train = len(X_train)\n",
        "len_val = len(X_val)\n",
        "\n",
        "for i in range(len_train):\n",
        "  X_train[i], Y_train[i] = tokenize_bert(X_train[i], Y_train[i])\n",
        "\n",
        "for i in range(len_val):\n",
        "  X_val[i], Y_val[i] = tokenize_bert(X_val[i], Y_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oMxNYeEEM9m"
      },
      "source": [
        "ones = 0\n",
        "zeros = 0\n",
        "total = 0\n",
        "for y in Y_train:\n",
        "  ones += np.sum(np.array(y))\n",
        "  zeros += len(y) - np.sum(np.array(y))\n",
        "  total += len(y)\n",
        "for y in Y_val:\n",
        "  ones += np.sum(np.array(y))\n",
        "  zeros += len(y) - np.sum(np.array(y))\n",
        "  total += len(y)\n",
        "# print(ones, zeros)\n",
        "class_weights = torch.tensor([zeros/zeros, zeros/ones], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LGxxvg1YAl_",
        "outputId": "475f5305-2874-47ce-9e69-e03fd172e788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.0000, 10.1637])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS4-S2UkrU6g"
      },
      "source": [
        "X_train_id = pad_sequences([tokenizer.convert_tokens_to_ids(text) for text in X_train], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_train_id = pad_sequences(Y_train, maxlen=MAX_LEN, value=CLASSES['[PAD]'], dtype='long', truncating='post', padding='post')\n",
        "X_val_id = pad_sequences([tokenizer.convert_tokens_to_ids(text) for text in X_val], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_val_id = pad_sequences(Y_val, maxlen=MAX_LEN, value=CLASSES['[PAD]'], dtype='long', truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-ZeHTxbtDXv"
      },
      "source": [
        "def get_attention_mask(x):\n",
        "  return([[(i!=0) for i in text] for text in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls4IdE3puP1f"
      },
      "source": [
        "attention_mask_train = get_attention_mask(X_train_id)\n",
        "attention_mask_val = get_attention_mask(X_val_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asbZ8r3RutFK"
      },
      "source": [
        "X_train_id = torch.tensor(X_train_id)\n",
        "Y_train_id = torch.tensor(Y_train_id)\n",
        "X_val_id = torch.tensor(X_val_id) \n",
        "Y_val_id = torch.tensor(Y_val_id)\n",
        "attention_mask_train = torch.tensor(attention_mask_train)\n",
        "attention_mask_val = torch.tensor(attention_mask_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QbjKgRGvfkx"
      },
      "source": [
        "train_data = TensorDataset(X_train_id, attention_mask_train, Y_train_id)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(X_val_id, attention_mask_val, Y_val_id)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ArycpaVRoP"
      },
      "source": [
        "configuration = BertConfig(hidden_dropout_prob=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEOOT0vz1jtH"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-frbk8B2-dr",
        "outputId": "b557f6de-6514-41a8-ad5f-8e1370ed1ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZHf22sB3ofL"
      },
      "source": [
        "FINE_TUNING = True\n",
        "if FINE_TUNING:\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'gamma', 'beta']\n",
        "  optimizer_grouped_parameters = [\n",
        "                                  {'params' : [p for n,p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate' : 0.01},\n",
        "                                  {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n",
        "else:\n",
        "  param_optimizer = list(model.classifier.named_parameters())\n",
        "  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpxC-F9G5QOR"
      },
      "source": [
        "epochs = 10\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42FMoENrD_r8"
      },
      "source": [
        "criterion = SelfAdjDiceLoss(alpha=0.2, gamma=1)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqywy8xeEpns"
      },
      "source": [
        "def get_text_lengths(masks):\n",
        "  lengths = []\n",
        "  for mask in masks:\n",
        "    lengths.append(torch.sum(mask).item())\n",
        "  return(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErI5p3SX6BSB",
        "outputId": "2b92cc4f-0c5d-4bdd-cda2-68d1f04f8ce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_loss, val_loss = [], []\n",
        "train_acc, val_acc = [], []\n",
        "train_f1, val_f1 = [], []\n",
        "\n",
        "l = 100\n",
        "\n",
        "for epoch in trange(epochs, desc = 'Epoch'):\n",
        "  print(epoch)\n",
        "  model.train()\n",
        "  t_loss, t_acc = 0, 0\n",
        "  predictions, true_labels = [], []\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    model.zero_grad()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    outputs = model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels = b_labels)\n",
        "    # loss = outputs[0]\n",
        "    active_logits = torch.masked_select(outputs[1], b_input_mask.unsqueeze(-1).repeat(1, 1, 2))\n",
        "    active_labels = torch.masked_select(b_labels, b_input_mask)\n",
        "    # print(active_labels.shape, active_labels)\n",
        "    loss = criterion(active_logits.view(-1, 2), active_labels)\n",
        "    loss.backward()\n",
        "    t_loss += loss.item()\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "    \n",
        "  print(f\"Train Loss : {t_loss/len(train_dataloader)}\")\n",
        "  train_loss.append(t_loss/len(train_dataloader))\n",
        "  pred_tags = [p_i for p, l in zip(predictions, true_labels)\n",
        "                                for p_i, l_i in zip(p, l) if l_i != 2]\n",
        "  valid_tags = [l_i for l in true_labels\n",
        "                                for l_i in l if l_i != 2]\n",
        "  train_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  train_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print(\"Train Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Train F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  print()\n",
        "\n",
        "  model.eval()\n",
        "  v_loss, v_accuracy = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "  for batch in val_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "      outputs = model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    \n",
        "    active_logits = torch.masked_select(outputs[1], b_input_mask.unsqueeze(-1).repeat(1, 1, 2))\n",
        "    active_labels = torch.masked_select(b_labels, b_input_mask)\n",
        "    # print(active_labels.shape, active_labels)\n",
        "    loss = criterion(active_logits.view(-1, 2), active_labels)\n",
        "    \n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    v_loss += loss.item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "    \n",
        "  \n",
        "  v_loss = v_loss/len(val_dataloader)\n",
        "  val_loss.append(v_loss)\n",
        "  if(v_loss < l):\n",
        "    l = v_loss\n",
        "    print(\"Model Checkpoint\")\n",
        "  torch.save(model, f'/content/drive/My Drive/model{epoch}.pt')\n",
        "    \n",
        "  print(f\"Validation Loss : {v_loss}\")\n",
        "  pred_tags = [p_i for p, l in zip(predictions, true_labels)\n",
        "                                for p_i, l_i in zip(p, l) if l_i != 2]\n",
        "  valid_tags = [l_i for l in true_labels\n",
        "                                for l_i in l if l_i != 2]\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  val_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  val_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss : 0.1794031058585914\n",
            "Train Accuracy: 0.919855172369362\n",
            "Train F1-Score: 0.3150534082149543\n",
            "\n",
            "Model Checkpoint\n",
            "Validation Loss : 0.17712823763489724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 1/10 [11:32<1:43:50, 692.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9214276341687443\n",
            "Validation F1-Score: 0.4378520465640255\n",
            "\n",
            "1\n",
            "Train Loss : 0.17618014785444708\n",
            "Train Accuracy: 0.929931258415529\n",
            "Train F1-Score: 0.48664212215614083\n",
            "\n",
            "Model Checkpoint\n",
            "Validation Loss : 0.17653920799493789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 2/10 [23:06<1:32:22, 692.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9231334470541923\n",
            "Validation F1-Score: 0.4600921658986175\n",
            "\n",
            "2\n",
            "Train Loss : 0.17435325811656957\n",
            "Train Accuracy: 0.9376687132374258\n",
            "Train F1-Score: 0.5622766140342941\n",
            "\n",
            "Validation Loss : 0.17798962339758873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 3/10 [34:40<1:20:52, 693.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9213226610681013\n",
            "Validation F1-Score: 0.4439910979228487\n",
            "\n",
            "3\n",
            "Train Loss : 0.1726886371070852\n",
            "Train Accuracy: 0.9450035111680916\n",
            "Train F1-Score: 0.6310534846029172\n",
            "\n",
            "Validation Loss : 0.17985217049717903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 4/10 [46:15<1:09:22, 693.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9046844246161921\n",
            "Validation F1-Score: 0.46588235294117647\n",
            "\n",
            "4\n",
            "Train Loss : 0.1708393052445551\n",
            "Train Accuracy: 0.9531210740953105\n",
            "Train F1-Score: 0.6998453129833969\n",
            "\n",
            "Validation Loss : 0.17861296832561493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 5/10 [57:50<57:50, 694.10s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9158115732843459\n",
            "Validation F1-Score: 0.45590230664857534\n",
            "\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9PqWJnDjbN4"
      },
      "source": [
        "conf = confusion_matrix(valid_tags, pred_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6shV_S5kfc1"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_cm = pd.DataFrame(conf, range(2), range(2))\n",
        "# plt.figure(figsize=(10,7))\n",
        "sns.set(font_scale=1.4) # for label size\n",
        "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}, fmt='d') # font size\n",
        "\n",
        "plt.show()\n",
        "# plt.savefig('bert.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrJ-YTOtY6Qm"
      },
      "source": [
        "epoch=[]\n",
        "for i in range(3):\n",
        "    epoch.append(i+1)\n",
        "sns.set(font_scale=1.4)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "dic={'Train_Loss':train_loss,'Val_Loss': val_loss}\n",
        "x=pd.DataFrame(dic)\n",
        "ax = sns.lineplot(data=x)\n",
        "ax.set(ylim=(0,0.8),xlabel='No. of epochs',ylabel='Loss',title='BERT')\n",
        "ax.set_xticks(range(5)) # <--- s\n",
        "ax.set_xticklabels(['1','2','3','4', '5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FuwlKR5hJU"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "epoch=[]\n",
        "for i in range(3):\n",
        "    epoch.append(i+1)\n",
        "sns.set(font_scale=1.4)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "dic={'Train_Acc':train_acc,'Val_Acc': val_acc}\n",
        "x=pd.DataFrame(dic)\n",
        "ax = sns.lineplot(data=x)\n",
        "ax.set(ylim=(0,1),xlabel='No. of epochs',ylabel='Acc',title='BERT')\n",
        "ax.set_xticks(range(5)) # <--- s\n",
        "ax.set_xticklabels(['1','2','3','4', '5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwA-3GHr5x_x"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "epoch=[]\n",
        "for i in range(3):\n",
        "    epoch.append(i+1)\n",
        "sns.set(font_scale=1.4)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "dic={'Train_F1_Score':train_f1,'Val_F1_Score': val_f1}\n",
        "x=pd.DataFrame(dic)\n",
        "ax = sns.lineplot(data=x)\n",
        "ax.set(ylim=(0,1),xlabel='No. of epochs',ylabel='F1 Score',title='BERT')\n",
        "ax.set_xticks(range(5)) # <--- s\n",
        "ax.set_xticklabels(['1','2','3','4', '5'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
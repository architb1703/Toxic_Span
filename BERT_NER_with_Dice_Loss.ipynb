{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT-NER with Dice Loss",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/archit/BERT_NER_with_Dice_Loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmhCJJpF225N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250c5018-379f-48ea-a87a-91fbef004881"
      },
      "source": [
        "!pip install transformers==2.6.0\n",
        "!pip install seqeval\n",
        "!pip install urllib3 --upgrade\n",
        "!pip install sadice"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.6.0 in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.16.23)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.1.94)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.19.23)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.23->boto3->transformers==2.6.0) (2.8.1)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.26.2\n",
            "    Uninstalling urllib3-1.26.2:\n",
            "      Successfully uninstalled urllib3-1.26.2\n",
            "Successfully installed urllib3-1.25.11\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Collecting urllib3\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.25.11\n",
            "    Uninstalling urllib3-1.25.11:\n",
            "      Successfully uninstalled urllib3-1.25.11\n",
            "Successfully installed urllib3-1.26.2\n",
            "Requirement already satisfied: sadice in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from sadice) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.0->sadice) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn4Uqbz-4-dJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604598a5-cf1a-4bcc-c4cb-79bb34830931"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torchtext import data\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "from sadice import SelfAdjDiceLoss\n",
        "\n",
        "torch.manual_seed(12)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmkRlyPK5Qtq"
      },
      "source": [
        "train_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/train.pkl'\n",
        "val_path = '/content/drive/My Drive/ToxicSpan_CS669V/BERT_Preprocess/val.pkl'\n",
        "\n",
        "with open(train_path, 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "  val_data = pickle.load(f)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WxA6XvA66Ha"
      },
      "source": [
        "val_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-EqtoyUhcpc"
      },
      "source": [
        "MAX_LEN = 500\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DePWwE_JoDu0"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnSsn5MXoTxT"
      },
      "source": [
        "X_train = train_data['token_final']\n",
        "X_val = val_data['token_final']\n",
        "Y_train = train_data['target_final']\n",
        "Y_val = val_data['target_final']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G8KiiEUTSdY"
      },
      "source": [
        "CLASSES = {'0':0, '1':1, '[PAD]':2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsYGVo5nouK0"
      },
      "source": [
        "def tokenize_bert(x, y):\n",
        "  sentence = []\n",
        "  labels = [0]\n",
        "  for word, label in zip(x, y):\n",
        "    tokenized_word = tokenizer.tokenize(word)\n",
        "    sentence.extend(tokenized_word)\n",
        "    labels.extend([label for i in range(len(tokenized_word))])\n",
        "  labels.append(0)\n",
        "  return(sentence, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3eyhTSSpPsa"
      },
      "source": [
        "len_train = len(X_train)\n",
        "len_val = len(X_val)\n",
        "\n",
        "for i in range(len_train):\n",
        "  X_train[i], Y_train[i] = tokenize_bert(X_train[i], Y_train[i])\n",
        "\n",
        "for i in range(len_val):\n",
        "  X_val[i], Y_val[i] = tokenize_bert(X_val[i], Y_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS4-S2UkrU6g"
      },
      "source": [
        "X_train_id = pad_sequences([tokenizer.encode(text) for text in X_train], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_train_id = pad_sequences(Y_train, maxlen=MAX_LEN, value=CLASSES['[PAD]'], dtype='long', truncating='post', padding='post')\n",
        "X_val_id = pad_sequences([tokenizer.encode(text) for text in X_val], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_val_id = pad_sequences(Y_val, maxlen=MAX_LEN, value=CLASSES['[PAD]'], dtype='long', truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-ZeHTxbtDXv"
      },
      "source": [
        "def get_attention_mask(x):\n",
        "  return([[(i!=0) for i in text] for text in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls4IdE3puP1f"
      },
      "source": [
        "attention_mask_train = get_attention_mask(X_train_id)\n",
        "attention_mask_val = get_attention_mask(X_val_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asbZ8r3RutFK"
      },
      "source": [
        "X_train_id = torch.tensor(X_train_id)\n",
        "Y_train_id = torch.tensor(Y_train_id)\n",
        "X_val_id = torch.tensor(X_val_id) \n",
        "Y_val_id = torch.tensor(Y_val_id)\n",
        "attention_mask_train = torch.tensor(attention_mask_train)\n",
        "attention_mask_val = torch.tensor(attention_mask_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QbjKgRGvfkx"
      },
      "source": [
        "train_data = TensorDataset(X_train_id, attention_mask_train, Y_train_id)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(X_val_id, attention_mask_val, Y_val_id)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEOOT0vz1jtH"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-frbk8B2-dr"
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZHf22sB3ofL"
      },
      "source": [
        "FINE_TUNING = True\n",
        "if FINE_TUNING:\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'gamma', 'beta']\n",
        "  optimizer_grouped_parameters = [\n",
        "                                  {'params' : [p for n,p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate' : 0.01},\n",
        "                                  {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n",
        "else:\n",
        "  param_optimizer = list(model.classifier.named_parameters())\n",
        "  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpxC-F9G5QOR"
      },
      "source": [
        "epochs = 10\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42FMoENrD_r8"
      },
      "source": [
        "criterion = SelfAdjDiceLoss(alpha=0.8, gamma=1)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqywy8xeEpns"
      },
      "source": [
        "def get_text_lengths(masks):\n",
        "  lengths = []\n",
        "  for mask in masks:\n",
        "    lengths.append(torch.sum(mask).item())\n",
        "  return(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErI5p3SX6BSB"
      },
      "source": [
        "train_loss, val_loss = [], []\n",
        "train_acc, val_acc = [], []\n",
        "train_f1, val_f1 = [], []\n",
        "\n",
        "l = 100\n",
        "\n",
        "for epoch in trange(epochs, desc = 'Epoch'):\n",
        "  print(epoch)\n",
        "  model.train()\n",
        "  t_loss, t_acc = 0, 0\n",
        "  predictions, true_labels = [], []\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    model.zero_grad()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    outputs = model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels = b_labels)\n",
        "    # loss = outputs[0]\n",
        "    active_logits = torch.masked_select(outputs[1], b_input_mask.unsqueeze(-1).repeat(1, 1, 2))\n",
        "    active_labels = torch.masked_select(b_labels, b_input_mask)\n",
        "    # print(active_labels.shape, active_labels)\n",
        "    loss = criterion(active_logits.view(-1, 2), active_labels)\n",
        "    loss.backward()\n",
        "    t_loss += loss.item()\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "    \n",
        "  print(f\"Train Loss : {t_loss/len(train_dataloader)}\")\n",
        "  train_loss.append(t_loss/len(train_dataloader))\n",
        "  pred_tags = [p_i for p, l in zip(predictions, true_labels)\n",
        "                                for p_i, l_i in zip(p, l) if l_i != 2]\n",
        "  valid_tags = [l_i for l in true_labels\n",
        "                                for l_i in l if l_i != 2]\n",
        "  train_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  train_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print(\"Train Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Train F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  print()\n",
        "\n",
        "  model.eval()\n",
        "  v_loss, v_accuracy = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "  for batch in val_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "      outputs = model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    \n",
        "    active_logits = torch.masked_select(outputs[1], b_input_mask.unsqueeze(-1).repeat(1, 1, 2))\n",
        "    active_labels = torch.masked_select(b_labels, b_input_mask)\n",
        "    # print(active_labels.shape, active_labels)\n",
        "    loss = criterion(active_logits.view(-1, 2), active_labels)\n",
        "    \n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    v_loss += loss.item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "    \n",
        "  \n",
        "  v_loss = v_loss/len(val_dataloader)\n",
        "  val_loss.append(v_loss)\n",
        "  if(v_loss < l):\n",
        "    l = v_loss\n",
        "    print(\"Model Checkpoint\")\n",
        "  torch.save(model, f'/content/drive/My Drive/model{epoch}.pt')\n",
        "    \n",
        "  print(f\"Validation Loss : {v_loss}\")\n",
        "  pred_tags = [p_i for p, l in zip(predictions, true_labels)\n",
        "                                for p_i, l_i in zip(p, l) if l_i != 2]\n",
        "  valid_tags = [l_i for l in true_labels\n",
        "                                for l_i in l if l_i != 2]\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  val_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  val_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9PqWJnDjbN4"
      },
      "source": [
        "conf = confusion_matrix(valid_tags, pred_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6shV_S5kfc1"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_cm = pd.DataFrame(conf, range(2), range(2))\n",
        "# plt.figure(figsize=(10,7))\n",
        "sns.set(font_scale=1.4) # for label size\n",
        "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}, fmt='d') # font size\n",
        "\n",
        "plt.show()\n",
        "# plt.savefig('bert.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrJ-YTOtY6Qm"
      },
      "source": [
        "epoch=[]\n",
        "for i in range(3):\n",
        "    epoch.append(i+1)\n",
        "sns.set(font_scale=1.4)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "dic={'Train_Loss':train_loss,'Val_Loss': val_loss}\n",
        "x=pd.DataFrame(dic)\n",
        "ax = sns.lineplot(data=x)\n",
        "ax.set(ylim=(0,0.5),xlabel='No. of epochs',ylabel='Loss',title='BERT')\n",
        "ax.set_xticks(range(5)) # <--- s\n",
        "ax.set_xticklabels(['1','2','3','4', '5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FuwlKR5hJU"
      },
      "source": [
        "epoch=[]\n",
        "for i in range(3):\n",
        "    epoch.append(i+1)\n",
        "sns.set(font_scale=1.4)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "dic={'Train_Acc':train_acc,'Val_Acc': val_acc}\n",
        "x=pd.DataFrame(dic)\n",
        "ax = sns.lineplot(data=x)\n",
        "ax.set(ylim=(0,1),xlabel='No. of epochs',ylabel='Acc',title='BERT')\n",
        "ax.set_xticks(range(5)) # <--- s\n",
        "ax.set_xticklabels(['1','2','3','4', '5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwA-3GHr5x_x"
      },
      "source": [
        "epoch=[]\n",
        "for i in range(3):\n",
        "    epoch.append(i+1)\n",
        "sns.set(font_scale=1.4)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "dic={'Train_F1_Score':train_f1,'Val_F1_Score': val_f1}\n",
        "x=pd.DataFrame(dic)\n",
        "ax = sns.lineplot(data=x)\n",
        "ax.set(ylim=(0,1),xlabel='No. of epochs',ylabel='F1 Score',title='BERT')\n",
        "ax.set_xticks(range(10)) # <--- s\n",
        "ax.set_xticklabels([str(i) for i in range(10)])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "from ast import literal_eval\n",
    "train = pd.read_csv(\"Dataset/tsd_train.csv\")\n",
    "train[\"spans\"] = train.spans.apply(literal_eval)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = dup of \"train\"\n",
    "corpus = train.copy()\n",
    "\n",
    "#generate toxic chars\n",
    "toxic = [ '' for i in range(len(corpus))]\n",
    "for i in range(0, len(corpus)):\n",
    "    string = corpus.text[i]\n",
    "    posi = corpus.spans[i]\n",
    "    for j in range(len(posi)):\n",
    "        toxic[i] += string[posi[j]]\n",
    "\n",
    "corpus[\"toxic_chars\"] = toxic\n",
    "del toxic\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding white spaces which are marked toxic\n",
    "def find_whitespace(string):\n",
    "    if not string:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(string.split())-1  \n",
    "\n",
    "corpus[\"toxic_whitespaces\"] = corpus.toxic_chars.apply(lambda x: find_whitespace(x))\n",
    "corpus[\"total_toxic_char\"] = corpus.toxic_chars.apply(lambda x: len(x))\n",
    "corpus[\"white_by_char\"] = 100*corpus['toxic_whitespaces']/(corpus['total_toxic_char']+.00000001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats of words and chars\n",
    "length_of_words = corpus['text'].apply(lambda x: len(x.split()))\n",
    "length_of_text = corpus['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'word_level':length_of_words.describe(), 'char_level': length_of_text.describe()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization and cacheing spans \n",
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "\n",
    "def tokens(string):\n",
    "    tokens = twt().tokenize(string)\n",
    "    return tokens \n",
    "\n",
    "def spans_of_tokens(string):\n",
    "    list_of_spans = list(twt().span_tokenize(string))\n",
    "    return list_of_spans\n",
    "\n",
    "corpus[\"tokenize\"] = corpus[\"text\"].apply(lambda x: tokens(x))\n",
    "corpus[\"spans_of_tokens\"] = corpus[\"text\"].apply(lambda x: spans_of_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 0, 1 notation\n",
    "\n",
    "def target_vector_(span, span_of_token):\n",
    "    target = []\n",
    "    for x,y in span_of_token:\n",
    "            if x in span:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "        \n",
    "    return target\n",
    "\n",
    "temp = []\n",
    "for i in range(len(corpus)):\n",
    "        temp.append(target_vector_(corpus.spans[i], corpus.spans_of_tokens[i]))\n",
    "        \n",
    "corpus[\"target\"] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokenize:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = corpus['text'].apply(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to delete empty tokens and their corresponding values\n",
    "def delete_null(zipped_tokens):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(corpus)):\n",
    "    temp.append(list(zip(corpus.tokenize[i], corpus.spans_of_tokens[i], corpus.target[i])))\n",
    "    \n",
    "corpus[\"zipped\"] = temp\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning tokens (in zipped)\n",
    "def clean_string(string):\n",
    "    #code to clean string\n",
    "    return string\n",
    "\n",
    "def cleaning(iterable):\n",
    "    for i in range(len(iterable)):\n",
    "        iterable[i][0] = clean_string(iterable[i][0])    \n",
    "    \n",
    "    return iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty tokens (in zipped)\n",
    "def remove_empty(iterable):\n",
    "    return list(filter(lambda x: x[0]!= None ,iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "corpus.zipped = corpus.zipped.apply(lambda x: cleaning(x))\n",
    "# remove nulls\n",
    "corus.zipped = corpus.zipped.apply(lambda x: remove_empty(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

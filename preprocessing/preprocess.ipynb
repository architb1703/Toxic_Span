{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "from ast import literal_eval\n",
    "train = pd.read_csv(\"/home/samyakj/Desktop/Toxic_Span/Dataset/tsd_train.csv\")\n",
    "train[\"spans\"] = train.spans.apply(literal_eval)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = dup of \"train\"\n",
    "corpus = train.copy()\n",
    "\n",
    "#generate toxic chars\n",
    "toxic = [ '' for i in range(len(corpus))]\n",
    "for i in range(0, len(corpus)):\n",
    "    string = corpus.text[i]\n",
    "    posi = corpus.spans[i]\n",
    "    for j in range(len(posi)):\n",
    "        toxic[i] += string[posi[j]]\n",
    "\n",
    "corpus[\"toxic_chars\"] = toxic\n",
    "del toxic\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding white spaces which are marked toxic\n",
    "def find_whitespace(string):\n",
    "    count = 0\n",
    "    for i in string:\n",
    "        if(i.isspace()):\n",
    "            count=count+1  \n",
    "    return count\n",
    "corpus[\"toxic_whitespaces\"] = corpus.toxic_chars.apply(lambda x: find_whitespace(x))\n",
    "corpus[\"total_toxic_char\"] = corpus.toxic_chars.apply(lambda x: len(x))\n",
    "corpus[\"white_by_char\"] = 100*corpus['toxic_whitespaces']/(corpus['total_toxic_char']+.00000001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats of words and chars\n",
    "length_of_words = corpus['text'].apply(lambda x: len(x.split()))\n",
    "length_of_text = corpus['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'word_level':length_of_words.describe(), 'char_level': length_of_text.describe()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization and cacheing spans(position of token)\n",
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "\n",
    "def tokens(string):\n",
    "    tokens = twt().tokenize(string)\n",
    "    return tokens \n",
    "\n",
    "def spans_of_tokens(string):\n",
    "    list_of_spans = list(twt().span_tokenize(string))\n",
    "    return list_of_spans\n",
    "\n",
    "corpus[\"tokenize\"] = corpus[\"text\"].apply(lambda x: tokens(x))\n",
    "corpus[\"spans_of_tokens\"] = corpus[\"text\"].apply(lambda x: spans_of_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 0, 1 notation\n",
    "\n",
    "def target_vector_(span, span_of_token):\n",
    "    target = []\n",
    "    for x,y in span_of_token:\n",
    "            if x in span:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "        \n",
    "    return target\n",
    "\n",
    "temp = []\n",
    "for i in range(len(corpus)):\n",
    "        temp.append(target_vector_(corpus.spans[i], corpus.spans_of_tokens[i]))\n",
    "        \n",
    "corpus[\"target\"] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped together -> token, their spans, and target\n",
    "temp = []\n",
    "for i in range(len(corpus)):\n",
    "    temp.append(list(zip(corpus.tokenize[i], corpus.spans_of_tokens[i], corpus.target[i])))\n",
    "\n",
    "for i in range(len(temp)):    \n",
    "    temp[i] = [list(ele) for ele in temp[i]]  \n",
    "\n",
    "corpus[\"zipped\"] = temp\n",
    "del temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### cleaning functions ####################\n",
    "# emoji removing function\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "#puntuation remover\n",
    "from string import punctuation\n",
    "def strip_punctuation(text):\n",
    "    return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "#convert accented text\n",
    "import unicodedata\n",
    "def convert_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "#expand contraction\n",
    "from contractions import CONTRACTION_MAP # from contractions.py\n",
    "import re \n",
    "\n",
    "# function to expand contractions\n",
    "def expand_contractions(text, map=CONTRACTION_MAP):\n",
    "    if text in CONTRACTION_MAP.keys():\n",
    "        return CONTRACTION_MAP[text]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# function to remove digits\n",
    "def remove_nums(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "# remove special chars\n",
    "def remove_spe_chars(text):\n",
    "    return re.sub('[\\W_]+', '', text)\n",
    "#################### cleaning functions ####################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning tokens (in zipped)\n",
    "def clean_string(string):\n",
    "    string = string.lower()#lowercasing\n",
    "    string = expand_contractions(string) #expand contraction\n",
    "    string = remove_nums(string)#remove digits\n",
    "    #numbers into words(could be done if required)# not require though\n",
    "    string = convert_accented_chars(string)#convert accented\n",
    "    string = deEmojify(string)#remove emoji\n",
    "    string = remove_spe_chars(string)#remove all puntuation\n",
    "    \n",
    "    return string\n",
    "\n",
    "def cleaning(iterable):\n",
    "    for i in range(len(iterable)):\n",
    "        iterable[i][0] = clean_string(iterable[i][0])    \n",
    "    \n",
    "    return iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty tokens (in zipped)\n",
    "def remove_empty(iterable):\n",
    "    iterable = list(filter(lambda x: x[0] != '', iterable))\n",
    "    return iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "corpus.zipped = corpus.zipped.apply(lambda x: cleaning(x))\n",
    "# remove nulls\n",
    "corpus.zipped = corpus.zipped.apply(lambda x: remove_empty(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus.to_csv('processed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddem_target(lis):\n",
    "    arr = []\n",
    "    for item in lis:\n",
    "        arr.append(item[2])\n",
    "    return arr\n",
    "\n",
    "def redeem_spans(lis):\n",
    "    arr = []\n",
    "    for item in lis:\n",
    "        arr.append(item[1])\n",
    "    return arr\n",
    "\n",
    "def reddem_token(lis):\n",
    "    arr = []\n",
    "    for item in lis:\n",
    "        arr.append(item[0])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['target_final'] = corpus.zipped.apply(lambda x : reddem_target(x))\n",
    "\n",
    "corpus['span_final'] = corpus.zipped.apply(lambda x : redeem_spans(x))\n",
    "\n",
    "corpus['token_final'] = corpus.zipped.apply(lambda x : reddem_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = corpus[['spans', 'text', 'target_final', 'span_final', 'token_final']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_pickle(\"./final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAug.ipynb",
      "provenance": [],
      "mount_file_id": "1C8ZPGNkD3nSAZwdokJT4INRrS2SpHJWZ",
      "authorship_tag": "ABX9TyN0GkhIayFsVoFLjUlMzl2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/archit/DataAug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-mj-uDcuHnU",
        "outputId": "83c2f035-2064-4193-d024-8f82f59e63b1"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f0736dd2fd2efc434f7eb08d641eb60510434791026ad610b91be5ed371d6810\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqkJwypX-EaR",
        "outputId": "f0096d27-24c7-4a88-fe50-d49c25cddfa5"
      },
      "source": [
        "# from transformers import pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import Counter, OrderedDict\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INBRmyLoR32p"
      },
      "source": [
        "# **Data Statistics**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbfcIiIYKudz"
      },
      "source": [
        "#Load data\n",
        "train_path = '/content/drive/My Drive/BERT_Preprocess/train.pkl'\n",
        "\n",
        "with open(train_path, 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "trial_path = '/content/drive/My Drive/BERT_Preprocess/trial.pkl'\n",
        "\n",
        "with open(trial_path, 'rb') as f:\n",
        "  trial_data = pickle.load(f)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiRWEy_7snH_"
      },
      "source": [
        "train_data = pd.concat([train_data, trial_data], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnOAUpT_L68Q"
      },
      "source": [
        "#Build vocabulary of toxic tokens\n",
        "\n",
        "toxic_tokens = []\n",
        "\n",
        "for token_arr, target_arr in zip(train_data['token_final'], train_data['target_final']):\n",
        "  for token, target in zip(token_arr, target_arr):\n",
        "    if(target==1):\n",
        "      toxic_tokens.append(token.lower())\n",
        "\n",
        "t = Counter(toxic_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii_PcxmhNavr"
      },
      "source": [
        "stopword_dict = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuR7199uCtI9"
      },
      "source": [
        "lexicon = pd.read_csv('/content/drive/My Drive/EnglishShortened.csv')\n",
        "valence_value = dict(zip(lexicon['Word'], lexicon['valence']))\n",
        "lexicon_vocab = list(lexicon['Word'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "hlddY_rkEflq",
        "outputId": "bf9b13c2-08d1-4ec5-88bc-f01069053de6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# len(t.values())\n",
        "plt.hist(list(t.values()), bins=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3.739e+03, 1.140e+02, 4.700e+01, 2.100e+01, 1.800e+01, 1.300e+01,\n",
              "        6.000e+00, 6.000e+00, 5.000e+00, 2.000e+00, 2.000e+00, 2.000e+00,\n",
              "        2.000e+00, 0.000e+00, 2.000e+00, 3.000e+00, 0.000e+00, 1.000e+00,\n",
              "        0.000e+00, 0.000e+00, 1.000e+00, 2.000e+00, 1.000e+00, 1.000e+00,\n",
              "        1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
              "        1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
              " array([  1. ,  10.7,  20.4,  30.1,  39.8,  49.5,  59.2,  68.9,  78.6,\n",
              "         88.3,  98. , 107.7, 117.4, 127.1, 136.8, 146.5, 156.2, 165.9,\n",
              "        175.6, 185.3, 195. , 204.7, 214.4, 224.1, 233.8, 243.5, 253.2,\n",
              "        262.9, 272.6, 282.3, 292. , 301.7, 311.4, 321.1, 330.8, 340.5,\n",
              "        350.2, 359.9, 369.6, 379.3, 389. , 398.7, 408.4, 418.1, 427.8,\n",
              "        437.5, 447.2, 456.9, 466.6, 476.3, 486. , 495.7, 505.4, 515.1,\n",
              "        524.8, 534.5, 544.2, 553.9, 563.6, 573.3, 583. , 592.7, 602.4,\n",
              "        612.1, 621.8, 631.5, 641.2, 650.9, 660.6, 670.3, 680. , 689.7,\n",
              "        699.4, 709.1, 718.8, 728.5, 738.2, 747.9, 757.6, 767.3, 777. ,\n",
              "        786.7, 796.4, 806.1, 815.8, 825.5, 835.2, 844.9, 854.6, 864.3,\n",
              "        874. , 883.7, 893.4, 903.1, 912.8, 922.5, 932.2, 941.9, 951.6,\n",
              "        961.3, 971. ]),\n",
              " <a list of 100 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvW-Vhmpbva3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10c06ef-0ebe-4530-d313-41861d650c74"
      },
      "source": [
        "t['stupid']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "971"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ralgOsIaOdP-"
      },
      "source": [
        "toxic_tokens = list(t.keys())\n",
        "\n",
        "for i in toxic_tokens:\n",
        "  if(i in stopword_dict):\n",
        "    del t[i]\n",
        "  elif(i in string.punctuation):\n",
        "    del t[i]\n",
        "  elif(t[i] < 5):\n",
        "    del t[i]\n",
        "  elif(len(i) <=2):\n",
        "    del t[i]\n",
        "  \n",
        "  if((i in lexicon_vocab) and (valence_value[i]>6)):\n",
        "    del t[i]\n",
        "  elif(i not in lexicon_vocab):\n",
        "    del t[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "MgJrGMsFkNuo",
        "outputId": "9d514a26-82c2-4bc9-a95c-dbeed0175ae3"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "      <th>target_final</th>\n",
              "      <th>span_final</th>\n",
              "      <th>token_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[7, 8, 9, 10]</td>\n",
              "      <td>Only a fool would not be against this presiden...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 4), (5, 6), (7, 11), (12, 17), (18, 21), ...</td>\n",
              "      <td>[Only, a, fool, would, not, be, against, this,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[9, 10, 11, 12, 13, 14, 15, 16, 17, 55, 56, 57...</td>\n",
              "      <td>cops are dumbasses anyways and any male from u...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 4), (5, 8), (9, 18), (19, 26), (27, 30), ...</td>\n",
              "      <td>[cops, are, dumbasses, anyways, and, any, male...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2, 3, 4, 5, 6, 7]</td>\n",
              "      <td>A stupid post by a troll.  Thanks for outing y...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[(0, 1), (2, 8), (9, 13), (14, 16), (17, 18), ...</td>\n",
              "      <td>[A, stupid, post, by, a, troll, Thanks, for, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n",
              "      <td>Most idiotic, heartless comment I think I've e...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[(0, 4), (5, 12), (12, 13), (14, 23), (24, 31)...</td>\n",
              "      <td>[Most, idiotic, ,, heartless, comment, I, thin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
              "      <td>Some of you might have, but if you supported H...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[(0, 4), (5, 7), (8, 11), (12, 17), (18, 22), ...</td>\n",
              "      <td>[Some, of, you, might, have, ,, but, if, you, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6346</th>\n",
              "      <td>[684, 685, 686, 687, 688, 689, 703, 704, 705, ...</td>\n",
              "      <td>As for Clinton, it's well known why Putin want...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 2), (3, 6), (7, 14), (14, 15), (16, 18), ...</td>\n",
              "      <td>[As, for, Clinton, ,, it, 's, well, known, why...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6347</th>\n",
              "      <td>[46, 47, 48, 49, 98, 99, 100, 101, 102]</td>\n",
              "      <td>Because the author with this steaming pile of ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(0, 7), (8, 11), (12, 18), (19, 23), (24, 28)...</td>\n",
              "      <td>[Because, the, author, with, this, steaming, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6348</th>\n",
              "      <td>[11, 12, 13, 14, 15, 16, 17, 18, 19]</td>\n",
              "      <td>Manifestly ludicrous.</td>\n",
              "      <td>[0, 1]</td>\n",
              "      <td>[(0, 10), (11, 20)]</td>\n",
              "      <td>[Manifestly, ludicrous]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6349</th>\n",
              "      <td>[21, 22, 23, 24, 25, 26]</td>\n",
              "      <td>No need ban.  Anyone stupid enough to go to N....</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[(0, 2), (3, 7), (8, 12), (14, 20), (21, 27), ...</td>\n",
              "      <td>[No, need, ban, Anyone, stupid, enough, to, go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6350</th>\n",
              "      <td>[276, 277, 278, 279, 280, 281, 282, 283, 284, ...</td>\n",
              "      <td>“Does this guy have anything better to do with...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[(1, 5), (6, 10), (11, 14), (15, 19), (20, 28)...</td>\n",
              "      <td>[Does, this, guy, have, anything, better, to, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6351 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  spans  ...                                        token_final\n",
              "0                                         [7, 8, 9, 10]  ...  [Only, a, fool, would, not, be, against, this,...\n",
              "1     [9, 10, 11, 12, 13, 14, 15, 16, 17, 55, 56, 57...  ...  [cops, are, dumbasses, anyways, and, any, male...\n",
              "2                                    [2, 3, 4, 5, 6, 7]  ...  [A, stupid, post, by, a, troll, Thanks, for, o...\n",
              "3                               [5, 6, 7, 8, 9, 10, 11]  ...  [Most, idiotic, ,, heartless, comment, I, thin...\n",
              "4     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  ...  [Some, of, you, might, have, ,, but, if, you, ...\n",
              "...                                                 ...  ...                                                ...\n",
              "6346  [684, 685, 686, 687, 688, 689, 703, 704, 705, ...  ...  [As, for, Clinton, ,, it, 's, well, known, why...\n",
              "6347            [46, 47, 48, 49, 98, 99, 100, 101, 102]  ...  [Because, the, author, with, this, steaming, p...\n",
              "6348               [11, 12, 13, 14, 15, 16, 17, 18, 19]  ...                            [Manifestly, ludicrous]\n",
              "6349                           [21, 22, 23, 24, 25, 26]  ...  [No, need, ban, Anyone, stupid, enough, to, go...\n",
              "6350  [276, 277, 278, 279, 280, 281, 282, 283, 284, ...  ...  [Does, this, guy, have, anything, better, to, ...\n",
              "\n",
              "[6351 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWQDoJ6iSacL"
      },
      "source": [
        "# **Augmentation-1 : Duplicate Removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L6T4YblPjI-"
      },
      "source": [
        "#Creating a new data set by removing non-toxic duplicates from each sentence\n",
        "\n",
        "LEN_DATA_ORIG = len(train_data['token_final'])\n",
        "augmented_data_1 = []\n",
        "\n",
        "for i in range(LEN_DATA_ORIG):\n",
        "  new_sample = []\n",
        "  for j,token in enumerate(train_data['token_final'][i]):\n",
        "    if(train_data['target_final'][i][j]==1):\n",
        "      new_sample.append(token)\n",
        "    elif(token not in new_sample):\n",
        "      new_sample.append(token)\n",
        "  if(len(new_sample) == len(train_data['token_final'][i])):\n",
        "    continue\n",
        "\n",
        "  new_spans = []\n",
        "  new_targets = []\n",
        "  counter = 0\n",
        "  prev = 0\n",
        "  for j in range(len(train_data['token_final'][i])):\n",
        "    if(train_data['token_final'][i][j] == new_sample[counter]):\n",
        "      new_targets.append(train_data['target_final'][i][j])\n",
        "      if(counter==0 or (new_sample[counter] in string.punctuation) or ((counter!=0 and j!=0) and (new_sample[counter]==train_data['token_final'][i][j] and train_data['span_final'][i][j][0]==train_data['span_final'][i][j-1][1]))):\n",
        "        new_spans.append([prev, prev+len(new_sample[counter])])\n",
        "        prev += len(new_sample[counter])\n",
        "      else:\n",
        "        new_spans.append([prev+1, prev+1+len(new_sample[counter])])\n",
        "        prev += len(new_sample[counter])+1\n",
        "      counter+=1    \n",
        "    if(counter == len(new_sample)):\n",
        "      break\n",
        "  new_offsets = set([])\n",
        "  prev = 0\n",
        "  curr = 0\n",
        "  for i in range(len(new_sample)):\n",
        "    if(new_targets[i]==1):\n",
        "      if(prev==1):\n",
        "        new_offsets.add(curr)\n",
        "      for k in range(new_spans[i][0], new_spans[i][1]):\n",
        "        new_offsets.add(k)\n",
        "    curr = new_spans[i][1]\n",
        "    prev = new_targets[i]\n",
        "  augmented_data_1.append([train_data['text'][i], sorted(list(new_offsets)), new_sample, new_spans, new_targets])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR_BC_jHE5pJ",
        "outputId": "9c37db7e-9d67-4fe3-925f-b95a5d5a6edf"
      },
      "source": [
        "len(augmented_data_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSWjOZEtWqzS"
      },
      "source": [
        "data_1 = pd.DataFrame(augmented_data_1, columns=['text', 'spans', 'token_final', 'span_final', 'target_final'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "OZjh8n0unWf9",
        "outputId": "8ccb4dcb-5132-42cf-c70e-1088f7f72e32"
      },
      "source": [
        "data_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>spans</th>\n",
              "      <th>token_final</th>\n",
              "      <th>span_final</th>\n",
              "      <th>target_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You can't believe a darn thing Trump says.</td>\n",
              "      <td>[9, 10, 11, 12, 13, 14, 15, 16, 17, 55, 56, 57...</td>\n",
              "      <td>[cops, are, dumbasses, anyways, and, any, male...</td>\n",
              "      <td>[[0, 4], [5, 8], [9, 18], [19, 26], [27, 30], ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I was talking about the stupid comments from t...</td>\n",
              "      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n",
              "      <td>[Most, idiotic, ,, heartless, comment, I, thin...</td>\n",
              "      <td>[[0, 4], [5, 12], [12, 13], [14, 23], [24, 31]...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In a sane world, he'd be tied to the glacis pl...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Well, stated, !, The, 'Donald, ', could, be, ...</td>\n",
              "      <td>[[0, 4], [5, 11], [11, 12], [13, 16], [17, 24]...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no the author just added all his own bullshit ...</td>\n",
              "      <td>[14, 15, 16, 17, 18, 24, 25, 26, 27, 28, 29]</td>\n",
              "      <td>[Ah, ,, to, be, fat, drunk, and, stupid]</td>\n",
              "      <td>[[0, 2], [2, 3], [4, 6], [7, 9], [10, 13], [14...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reposting for second time, as my previous two ...</td>\n",
              "      <td>[37, 38, 39, 40, 41, 42, 43, 44]</td>\n",
              "      <td>[no, the, author, just, added, all, his, own, ...</td>\n",
              "      <td>[[0, 2], [3, 6], [7, 13], [14, 18], [19, 24], ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4471</th>\n",
              "      <td>How the innocents suffered  because of the wit...</td>\n",
              "      <td>[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 4...</td>\n",
              "      <td>[You, do, n't, have, to, wonder, Beav, ;, you,...</td>\n",
              "      <td>[[0, 3], [4, 6], [6, 9], [10, 14], [15, 17], [...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4472</th>\n",
              "      <td>The motorcycle on shoulder lane is just plain ...</td>\n",
              "      <td>[546, 547, 548, 549, 550, 551, 558, 559, 560, ...</td>\n",
              "      <td>[As, for, Clinton, ,, it, 's, well, known, why...</td>\n",
              "      <td>[[0, 2], [3, 6], [7, 14], [14, 15], [16, 18], ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4473</th>\n",
              "      <td>Darn!  Why does it seem that lawmakers are con...</td>\n",
              "      <td>[46, 47, 48, 49, 94, 95, 96, 97, 98]</td>\n",
              "      <td>[Because, the, author, with, this, steaming, p...</td>\n",
              "      <td>[[0, 7], [8, 11], [12, 18], [19, 23], [24, 28]...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4474</th>\n",
              "      <td>\"Quebec is an almost pathologically alienated ...</td>\n",
              "      <td>[19, 20, 21, 22, 23, 24]</td>\n",
              "      <td>[No, need, ban, Anyone, stupid, enough, to, go...</td>\n",
              "      <td>[[0, 2], [3, 7], [8, 11], [12, 18], [19, 25], ...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4475</th>\n",
              "      <td>Sounds like a horrific case of stupidity. If y...</td>\n",
              "      <td>[235, 236, 237, 238, 239, 240, 241, 242, 243, ...</td>\n",
              "      <td>[Does, this, guy, have, anything, better, to, ...</td>\n",
              "      <td>[[0, 4], [5, 9], [10, 13], [14, 18], [19, 27],...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4476 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...                                       target_final\n",
              "0            You can't believe a darn thing Trump says.  ...  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
              "1     I was talking about the stupid comments from t...  ...            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "2     In a sane world, he'd be tied to the glacis pl...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3     no the author just added all his own bullshit ...  ...                           [0, 0, 0, 0, 0, 1, 0, 1]\n",
              "4     Reposting for second time, as my previous two ...  ...                  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
              "...                                                 ...  ...                                                ...\n",
              "4471  How the innocents suffered  because of the wit...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, ...\n",
              "4472  The motorcycle on shoulder lane is just plain ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4473  Darn!  Why does it seem that lawmakers are con...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
              "4474  \"Quebec is an almost pathologically alienated ...  ...         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "4475  Sounds like a horrific case of stupidity. If y...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[4476 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj4nZjQmW0o_"
      },
      "source": [
        "pd.to_pickle(data_1, '/content/drive/My Drive/augmented_train_data_1.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic9vHCsQSvcP"
      },
      "source": [
        "# **Augmentation-2 : Random Masking**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbKlmit8LMIe"
      },
      "source": [
        "#Creating a new data set by randomly masking X% non-toxic tokens from the sentences\n",
        "\n",
        "LEN_DATA_ORIG = len(train_data['token_final'])\n",
        "augmented_data_2 = []\n",
        "\n",
        "for i in range(LEN_DATA_ORIG):\n",
        "  l = len(train_data['token_final'][i])\n",
        "  if(l <= 10 or sum(train_data['target_final'][i])==len(train_data['token_final'][i])):\n",
        "    continue\n",
        "  new_sample = []\n",
        "  new_spans = []\n",
        "  new_targets = []\n",
        "  new_offsets = []\n",
        "  prev = 0\n",
        "  curr = 0\n",
        "  flag = 0\n",
        "  threshold = random.uniform(0.1, 0.2)    #Setting the percentage hyperparameter for masking\n",
        "  for j in range(l):\n",
        "    if(train_data['target_final'][i][j]==1):\n",
        "      new_sample.append(train_data['token_final'][i][j])\n",
        "      new_targets.append(1)\n",
        "      if(len(new_sample)==1 or (flag==1 and train_data['span_final'][i][j-1][1]==train_data['span_final'][i][j][0])):\n",
        "        new_spans.append([curr, curr+len(train_data['token_final'][i][j])])\n",
        "        curr += len(train_data['token_final'][i][j])\n",
        "      else:\n",
        "        curr += 1\n",
        "        new_spans.append([curr, curr + len(train_data['token_final'][i][j])])\n",
        "        curr += len(train_data['token_final'][i][j])\n",
        "      if(j!=0 and prev==1):\n",
        "        new_offsets.extend(k for k in range(new_spans[-2][1], new_spans[-1][0]))\n",
        "      new_offsets.extend(k for k in range(new_spans[-1][0], new_spans[-1][1]))\n",
        "      flag = 1\n",
        "      prev = 1\n",
        "    else:\n",
        "      u = random.uniform(0,1)\n",
        "      if(u<threshold):\n",
        "        flag = 0\n",
        "        prev = 0\n",
        "        continue\n",
        "      new_sample.append(train_data['token_final'][i][j])\n",
        "      new_targets.append(0)\n",
        "      if(len(new_sample)==1 or (flag==1 and train_data['span_final'][i][j-1][1]==train_data['span_final'][i][j][0])):\n",
        "        new_spans.append([curr, curr+len(train_data['token_final'][i][j])])\n",
        "        curr += len(train_data['token_final'][i][j])\n",
        "      else:\n",
        "        curr += 1\n",
        "        new_spans.append([curr, curr + len(train_data['token_final'][i][j])])\n",
        "        curr += len(train_data['token_final'][i][j])\n",
        "      flag = 1\n",
        "      prev = 0\n",
        "  if(new_sample == train_data['token_final'][i]):\n",
        "    continue\n",
        "  augmented_data_2.append([train_data['text'][i], new_offsets, new_sample, new_spans, new_targets])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6cQXrN8Vt1d",
        "outputId": "f27d46b2-0e29-4f52-8e19-745a53c57f69"
      },
      "source": [
        "len(augmented_data_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5498"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYRjUTtBVygq",
        "outputId": "e1f13792-dce5-44a2-9e7d-d099d4d89d61"
      },
      "source": [
        "augmented_data_2[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Most idiotic, heartless comment I think I've ever read. \\n   You need help.\",\n",
              " [0, 1, 2, 3, 4, 5, 6],\n",
              " ['idiotic', ',', 'heartless', 'comment', 'I', 'think', 'I', 'ever', 'help'],\n",
              " [[0, 7],\n",
              "  [7, 8],\n",
              "  [9, 18],\n",
              "  [19, 26],\n",
              "  [27, 28],\n",
              "  [29, 34],\n",
              "  [35, 36],\n",
              "  [37, 41],\n",
              "  [42, 46]],\n",
              " [1, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQcq-eQQe6yK"
      },
      "source": [
        "data_2 = pd.DataFrame(augmented_data_2, columns=['text', 'spans', 'token_final', 'span_final', 'target_final'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgywFQR3356V"
      },
      "source": [
        "train_data = data_2   #To apply multiple augmentation methods"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVhcQ9JWeyn6"
      },
      "source": [
        "pd.to_pickle(data_2, '/content/drive/My Drive/augmented_train_data_2.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1USbd9zmTISv"
      },
      "source": [
        "# **Augmentation-3 : Random Swapping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKM8oRwqyIDf"
      },
      "source": [
        "#Creating new dataset using random swaps\n",
        "#Swaps are only made between tokens with same label\n",
        "\n",
        "LEN_DATA_ORIG = len(train_data['token_final'])\n",
        "augmented_data_3 = []\n",
        "\n",
        "for i in range(LEN_DATA_ORIG):\n",
        "\n",
        "  new_sample = train_data['token_final'][i]\n",
        "  new_targets = train_data['target_final'][i]\n",
        "  l = len(new_sample)\n",
        "  x = random.randint(10, 20)    #Setting hyperparameter for no. of random swaps\n",
        "  n = l//x\n",
        "  if(n==0):\n",
        "    continue\n",
        "  count = 0\n",
        "  while(count < n):\n",
        "    a = random.randint(0, l-1)\n",
        "    b = random.randint(0, l-1)\n",
        "    if((a==b) or train_data['target_final'][i][a]!=train_data['target_final'][i][b]):\n",
        "      continue\n",
        "    new_sample[a], new_sample[b] = new_sample[b], new_sample[a]\n",
        "    count+=1\n",
        "  augmented_data_3.append([new_sample, new_targets])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXeWVXunhXRP"
      },
      "source": [
        "data_3 = pd.DataFrame(augmented_data_3, columns=['token_final', 'target_final'])\n",
        "pd.to_pickle(data_3, '/content/drive/My Drive/augmented_train_data_6.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "712bO-0vPurU",
        "outputId": "2895a576-4358-49f0-9958-53046f4cacd2"
      },
      "source": [
        "data_3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token_final</th>\n",
              "      <th>target_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Only, would, fool, a, not, be, against, this,...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[pigjesus, dumbasses, anyways, and, any, male,...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[banty, stated, !, The, 'Donald, ', could, be,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[while, ,, the, focus, is, standing, the, whit...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[another, Trump, sycophant, in, lying, boot, l...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>[Are, a, Woman, ?, a, ?, Then, why, are, you, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5009</th>\n",
              "      <td>[this, a, moron, Alaska, is, own, enemy, on, t...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>[But, Trump, 's, not, bluffing, what, go, scor...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5011</th>\n",
              "      <td>[Ca, believe, the, this, of, knowledge, and, t...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5012</th>\n",
              "      <td>[Unless, there, is, in, ), contract, saying, o...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5013 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            token_final                                       target_final\n",
              "0     [Only, would, fool, a, not, be, against, this,...   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "1     [pigjesus, dumbasses, anyways, and, any, male,...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
              "2     [banty, stated, !, The, 'Donald, ', could, be,...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3     [while, ,, the, focus, is, standing, the, whit...  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...\n",
              "4     [another, Trump, sycophant, in, lying, boot, l...  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "...                                                 ...                                                ...\n",
              "5008  [Are, a, Woman, ?, a, ?, Then, why, are, you, ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "5009  [this, a, moron, Alaska, is, own, enemy, on, t...  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "5010  [But, Trump, 's, not, bluffing, what, go, scor...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "5011  [Ca, believe, the, this, of, knowledge, and, t...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "5012  [Unless, there, is, in, ), contract, saying, o...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[5013 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}
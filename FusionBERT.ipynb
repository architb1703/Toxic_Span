{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FusionBERT.ipynb",
      "provenance": [],
      "mount_file_id": "1cL2WAgEa4gTVJEXkQWrjv_Vqce5ZvpaK",
      "authorship_tag": "ABX9TyN7RDfLUBeJsdzX/Afecqhr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/Abhay/FusionBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yJ1nLR5N0eJ",
        "outputId": "a05e7702-4214-4e23-f689-ebca7098955f"
      },
      "source": [
        "!pip install transformers==2.6.0\n",
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.6.0 in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.0.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.16.36)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.17.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.19.36)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.36->boto3->transformers==2.6.0) (2.8.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXRdGjhiRfHj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torchtext import data\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertModel, AdamW, BertForTokenClassification, BertPreTrainedModel, BertConfig\n",
        "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForTokenClassification\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from seqeval.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JmDqUwMRmiy"
      },
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(12)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcN5VNDrRpT1"
      },
      "source": [
        "train_path = 'drive/My Drive/Ensemble/train.pkl'\n",
        "val_path = 'drive/My Drive/Ensemble/val.pkl'\n",
        "\n",
        "with open(train_path, 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "  val_data = pickle.load(f)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aITkl6nOS6jl"
      },
      "source": [
        "X_train = train_data['token_final']\n",
        "X_val = val_data['token_final']\n",
        "Y_train = train_data['target_final']\n",
        "Y_val = val_data['target_final']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bxiWMjJS9Vl"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a1tijHXTCBm"
      },
      "source": [
        "def tokenize_bert(x, y):\n",
        "  sentence = []\n",
        "  labels = [0]\n",
        "  for word, label in zip(x, y):\n",
        "    tokenized_word = tokenizer.tokenize(word)\n",
        "    sentence.extend(tokenized_word)\n",
        "    labels.extend([label for i in range(len(tokenized_word))])\n",
        "  labels.append(0)\n",
        "  return(sentence, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gfMvn5CTCuG"
      },
      "source": [
        "  len_train = len(X_train)\n",
        "  len_val = len(X_val)\n",
        "\n",
        "  for i in range(len_train):\n",
        "    X_train[i], Y_train[i] = tokenize_bert(X_train[i], Y_train[i])\n",
        "\n",
        "  for i in range(len_val):\n",
        "    X_val[i], Y_val[i] = tokenize_bert(X_val[i], Y_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iTLQ747Ulk4"
      },
      "source": [
        "ones = 0\n",
        "zeros = 0\n",
        "total = 0\n",
        "for y in Y_train:\n",
        "  ones += np.sum(np.array(y))\n",
        "  zeros += len(y) - np.sum(np.array(y))\n",
        "  total += len(y)\n",
        "for y in Y_val:\n",
        "  ones += np.sum(np.array(y))\n",
        "  zeros += len(y) - np.sum(np.array(y))\n",
        "  total += len(y)\n",
        "class_weights = torch.tensor([zeros/zeros, zeros/ones], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz-CRa2TV1d1"
      },
      "source": [
        "CLASSES = {'0':0, '1':1, '[PAD]':2}\n",
        "MAX_LEN = 500\n",
        "BATCH_SIZE = 4\n",
        "#Convert tokens to token_ids for bert_model, add special tokens to the sequences, pad the sequences\n",
        "X_train_id = pad_sequences([tokenizer.encode(text) for text in X_train], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_train_id = pad_sequences(Y_train, maxlen=MAX_LEN, value=CLASSES['[PAD]'], dtype='long', truncating='post', padding='post')\n",
        "X_val_id = pad_sequences([tokenizer.encode(text) for text in X_val], maxlen = MAX_LEN, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "Y_val_id = pad_sequences(Y_val, maxlen=MAX_LEN, value=CLASSES['[PAD]'], dtype='long', truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXYf5_EgV319"
      },
      "source": [
        "def get_attention_mask(x):\n",
        "  return([[(i!=0) for i in text] for text in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVwkJBzYV5pG"
      },
      "source": [
        "attention_mask_train = get_attention_mask(X_train_id)\n",
        "attention_mask_val = get_attention_mask(X_val_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1RD40VOV7yc"
      },
      "source": [
        "X_train_id = torch.tensor(X_train_id)\n",
        "Y_train_id = torch.tensor(Y_train_id)\n",
        "X_val_id = torch.tensor(X_val_id)\n",
        "Y_val_id = torch.tensor(Y_val_id)\n",
        "attention_mask_train = torch.tensor(attention_mask_train)\n",
        "attention_mask_val = torch.tensor(attention_mask_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxJrV6PbV-NN"
      },
      "source": [
        "train_data = TensorDataset(X_train_id, attention_mask_train, Y_train_id)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(X_val_id, attention_mask_val, Y_val_id)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtNkeeBkWA68"
      },
      "source": [
        "class BertEnsemble(BertPreTrainedModel):\n",
        "  def __init__(self,config,*args,**kwargs):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    self.bert_model_1 = BertModel.from_pretrained('bert-base-cased', output_attentions = False, output_hidden_states = False)\n",
        "    self.bert_model_2 = BertModel.from_pretrained('bert-base-cased', output_attentions = False, output_hidden_states = False)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    self.init_weights()\n",
        "  def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        # output_attentions=None,\n",
        "        # output_hidden_states=None,\n",
        "        # return_dict=None,\n",
        "    ):\n",
        "    outputs = self.bert_model_1(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            # output_attentions=output_attentions,\n",
        "            # output_hidden_states=output_hidden_states,\n",
        "            # return_dict=return_dict,\n",
        "        )\n",
        "    outputs_2 = self.bert_model_2(\n",
        "          input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids,\n",
        "          position_ids=position_ids,\n",
        "          head_mask=head_mask,\n",
        "          inputs_embeds=inputs_embeds,\n",
        "          # output_attentions=output_attentions,\n",
        "          # output_hidden_states=output_hidden_states,\n",
        "          # return_dict=return_dict,\n",
        "      )\n",
        "    sequence_output = outputs[0]\n",
        "    sequence_output2 = outputs_2[0]\n",
        "    sequence_output = self.dropout(sequence_output)\n",
        "    sequence_output2 = self.dropout(sequence_output2)\n",
        "    logits = self.classifier(sequence_output+sequence_output2)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        # Only keep active parts of the loss\n",
        "        if attention_mask is not None:\n",
        "            active_loss = attention_mask.view(-1) == 1\n",
        "            active_logits = logits.view(-1, self.num_labels)\n",
        "            active_labels = torch.where(\n",
        "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "            )\n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "        else:\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "  \n",
        "    output = (logits,) + outputs[2:]\n",
        "    return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOEnpd93WXhK",
        "outputId": "0eee1a22-318a-4dce-9bfe-1a648c67f0d0"
      },
      "source": [
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "config = BertConfig()\n",
        "model = BertEnsemble(config)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertEnsemble(\n",
              "  (bert_model_1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_model_2): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RSpX_1pXPgl"
      },
      "source": [
        "train_loss, val_loss = [], []\n",
        "train_acc, val_acc = [], []\n",
        "train_f1, val_f1 = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnKkveybXcH5"
      },
      "source": [
        "FINE_TUNING = True\n",
        "if FINE_TUNING:\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'gamma', 'beta']\n",
        "  optimizer_grouped_parameters = [\n",
        "                                  {'params' : [p for n,p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate' : 0.01},\n",
        "                                  {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n",
        "else:\n",
        "  param_optimizer = list(model.classifier.named_parameters())\n",
        "  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wk54KdmXeO-"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 4\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSMHnrZYXfuK",
        "outputId": "8bd94afc-d184-499c-dde3-19291f18404f"
      },
      "source": [
        "#Training code\n",
        "\n",
        "train_loss, val_loss = [], []\n",
        "train_acc, val_acc = [], []\n",
        "train_f1, val_f1 = [], []\n",
        "\n",
        "l = 100\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "criterion = criterion.to(device)\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  model.train()\n",
        "  t_loss, t_acc = 0, 0\n",
        "  predictions, true_labels = [], []\n",
        "  for step, batch in enumerate(tqdm(train_dataloader, desc='Train')):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    model.zero_grad()\n",
        "    outputs = model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels = b_labels)\n",
        "    # loss = outputs[0]\n",
        "\n",
        "    # Code for using weighted cross-entropy loss\n",
        "    active_loss = b_input_mask.view(-1) == 1\n",
        "    active_logits = outputs[1].view(-1, 2)\n",
        "    active_labels = torch.where(active_loss, b_labels.view(-1), torch.tensor(criterion.ignore_index).type_as(b_labels))\n",
        "    loss = criterion(active_logits, active_labels)\n",
        "\n",
        "    loss.backward()\n",
        "    t_loss += loss.item()\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "    \n",
        "  print(f\"Train Loss : {t_loss/len(train_dataloader)}\")\n",
        "  train_loss.append(t_loss/len(train_dataloader))\n",
        "  pred_tags = [[p_i for p_i, l_i in zip(p,l) if l_i!=2] for p,l in zip(predictions, true_labels)]\n",
        "  pred_tags = [p_i for p in pred_tags for p_i in p[1:-1]]\n",
        "  # pred_tags = [p_i for p, l in zip(predictions, true_labels)\n",
        "  #                               for p_i, l_i in zip(p, l) if l_i != 2]\n",
        "  # valid_tags = [l_i for l in true_labels\n",
        "  #                               for l_i in l if l_i != 2]\n",
        "  valid_tags = [[l_i for l_i in l if l_i!=2] for l in true_labels]\n",
        "  valid_tags = [l_i for l in valid_tags for l_i in l[1:-1]]\n",
        "  train_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  train_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print(\"Train Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Train F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  print()\n",
        "\n",
        "  model.eval()\n",
        "  v_loss, v_accuracy = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "  for batch in tqdm(val_dataloader, desc=\"Val\"):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_id, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "      outputs = model(b_input_id, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    \n",
        "    # loss = outputs[0]\n",
        "\n",
        "    active_loss = b_input_mask.view(-1) == 1\n",
        "    active_logits = outputs[1].view(-1, 2)\n",
        "    active_labels = torch.where(active_loss, b_labels.view(-1), torch.tensor(criterion.ignore_index).type_as(b_labels))\n",
        "    loss = criterion(active_logits, active_labels)\n",
        "    \n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    v_loss += loss.item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "  \n",
        "  v_loss = v_loss/len(val_dataloader)\n",
        "  val_loss.append(v_loss)\n",
        "  if(v_loss < l):\n",
        "    l = v_loss\n",
        "    print(\"Model Checkpoint\")\n",
        "  torch.save(model, f'drive/MyDrive/Ensemble/ensemble{epoch}.pt')\n",
        "  \n",
        "  print(f\"Validation Loss : {v_loss}\")\n",
        "  pred_tags = [[p_i for p_i,l_i in zip(p,l) if l_i!=2] for p,l in zip(predictions, true_labels)]\n",
        "  pred_tags = [p_i for p in pred_tags for p_i in p[1:-1]]\n",
        "  # pred_tags = [p_i for p, l in zip(predictions, true_labels)\n",
        "  #                               for p_i, l_i in zip(p, l) if l_i != 2]\n",
        "  # valid_tags = [l_i for l in true_labels\n",
        "  #                               for l_i in l if l_i != 2]\n",
        "  valid_tags = [[l_i for l_i in l if l_i!=2] for l in true_labels]\n",
        "  valid_tags = [l_i for l in valid_tags for l_i in l[1:-1]]\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "  print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  val_acc.append(accuracy_score(pred_tags, valid_tags))\n",
        "  val_f1.append(f1_score(pred_tags, valid_tags))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rTrain:   0%|          | 0/1588 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Train: 100%|| 1588/1588 [22:12<00:00,  1.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss : 0.5842492848691021\n",
            "Train Accuracy: 0.7976126501330038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rVal:   0%|          | 0/199 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train F1-Score: 0.31373710224813794\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Val: 100%|| 199/199 [00:55<00:00,  3.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Checkpoint\n",
            "Validation Loss : 0.5077500883209046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTrain:   0%|          | 0/1588 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7726264479557466\n",
            "Validation F1-Score: 0.34513762915056395\n",
            "\n",
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train: 100%|| 1588/1588 [22:11<00:00,  1.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss : 0.48944597030222714\n",
            "Train Accuracy: 0.8624452535131795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rVal:   0%|          | 0/199 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train F1-Score: 0.4214169668715123\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Val: 100%|| 199/199 [00:55<00:00,  3.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss : 0.5736969535299881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTrain:   0%|          | 0/1588 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8973628720869732\n",
            "Validation F1-Score: 0.45236703682057283\n",
            "\n",
            "Epoch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train: 100%|| 1588/1588 [22:10<00:00,  1.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss : 0.4602065501729923\n",
            "Train Accuracy: 0.8745096326947362\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rVal:   0%|          | 0/199 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train F1-Score: 0.45619805842199485\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Val: 100%|| 199/199 [00:55<00:00,  3.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss : 0.5189071446657181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTrain:   0%|          | 0/1588 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8764137251143304\n",
            "Validation F1-Score: 0.4370712236497443\n",
            "\n",
            "Epoch 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train: 100%|| 1588/1588 [22:13<00:00,  1.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss : 0.43500436465455317\n",
            "Train Accuracy: 0.8872860520729774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rVal:   0%|          | 0/199 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train F1-Score: 0.49184597447040473\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Val: 100%|| 199/199 [00:55<00:00,  3.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss : 0.5732098550315777\n",
            "Validation Accuracy: 0.8699783662403812\n",
            "Validation F1-Score: 0.4286401925391095\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS8ANI73YOWQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reOL-SYqv8Qq"
      },
      "source": [
        "import pickle\n",
        "path ='drive/MyDrive/Ensemble/processedtest.pkl'\n",
        "with open(path,'rb') as f:\n",
        "  final_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYupPatrv8kF"
      },
      "source": [
        "use_tokens=final_data['token_final'].tolist()\n",
        "use_spans=final_data['span_final'].tolist()\n",
        "use_text=final_data['text'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRXTGXtcv8pl"
      },
      "source": [
        "class SpanAvgF1():\n",
        "  def __init__(self, model, tokenizer, X, spans, maxlen=500):\n",
        "    self.model = model\n",
        "    self.model.cuda()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.maxlen = maxlen\n",
        "    self.X = X\n",
        "    self.spans = spans\n",
        "    \n",
        "    self.prepare_data()\n",
        "\n",
        "    self.X = torch.tensor(self.X)\n",
        "    self.attention_mask = torch.tensor(self.attention_mask)\n",
        "\n",
        "    data = TensorDataset(self.X, self.attention_mask)\n",
        "    self.dataloader = DataLoader(data, batch_size=16, shuffle=False)\n",
        "  \n",
        "  def tokenize_data(self, x, s):\n",
        "    sentence = []\n",
        "    spans = []\n",
        "    for i in range(len(x)):\n",
        "      word = x[i]\n",
        "      # label = y[i]\n",
        "      tokenized_word = self.tokenizer.tokenize(word)\n",
        "      sentence.extend(tokenized_word)\n",
        "      curr = s[i][0]\n",
        "      spans.append([curr, curr+len(tokenized_word[0])])\n",
        "      curr += len(tokenized_word[0])\n",
        "      for j in range(len(tokenized_word)-1):\n",
        "        spans.append([curr, curr+len(tokenized_word[j+1])-2])\n",
        "        curr += len(tokenized_word[j+1])-2\n",
        "      spans[-1][-1] = s[i][1]\n",
        "\n",
        "    return(sentence, spans)\n",
        "\n",
        "  def get_attention_mask(self, x):\n",
        "    return([[(i!=0) for i in text] for text in x])\n",
        "\n",
        "  def prepare_data(self):\n",
        "    for i in range(len(self.X)):\n",
        "      self.X[i], self.spans[i] = self.tokenize_data(self.X[i], self.spans[i])\n",
        "    self.X = pad_sequences([tokenizer.encode(text) for text in self.X], maxlen = self.maxlen, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "    # self.y = pad_sequences(self.y, maxlen=self.maxlen, value=2, dtype='long', truncating='post', padding='post')\n",
        "    self.attention_mask = self.get_attention_mask(self.X)\n",
        "  \n",
        "  def get_text_lengths(self, masks):\n",
        "    lengths = []\n",
        "    for mask in masks:\n",
        "      lengths.append(torch.sum(mask).item())\n",
        "    return(lengths)\n",
        "\n",
        "  def evaluate(self):\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in self.dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_id, b_input_mask = batch\n",
        "      # b_input_id, b_input_mask, b_labels = batch\n",
        "      self.model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = self.model(b_input_id, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = outputs[0].detach().cpu().numpy()\n",
        "      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "      true_labels.extend(b_input_mask)\n",
        "    \n",
        "    self.pred_tags = [[p_i for p_i, l_i in zip(p, l) if l_i != 0][1:-1] for p, l in zip(predictions, true_labels)]\n",
        "    # else:\n",
        "    #   self.pred_tags = predictions\n",
        "    # self.pred_tags = predictions\n",
        "    return self.pred_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlILDVTUwHY3"
      },
      "source": [
        "X = use_tokens.copy()\n",
        "spans = use_spans.copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd3_jeAXwJq0"
      },
      "source": [
        "model_path='drive/MyDrive/Ensemble/ensemble1.pt'\n",
        "modelorg = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "# model= modelorg\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG7dcy7AwMPl"
      },
      "source": [
        "model=modelorg\n",
        "metric = SpanAvgF1(model, tokenizer, X, spans)\n",
        "out=metric.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u1J0c55wnUi"
      },
      "source": [
        "predicted_spans=[]\n",
        "for i in range(len(spans)):\n",
        "  s = [0 for k in range(len(use_text[i]))]\n",
        "  prev = 0\n",
        "  for j in range(len(spans[i])):\n",
        "    for k in range(spans[i][j][0], spans[i][j][1]):\n",
        "      s[k] = out[i][j]\n",
        "    if(prev==1 and out[i][j]==1):\n",
        "      for l in range(spans[i][j-1][1], spans[i][j][0]):\n",
        "        s[l] = 1\n",
        "    prev = out[i][j]\n",
        "  predicted_spans.append(s)\n",
        "sp1=predicted_spans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVCg3xilwqJK"
      },
      "source": [
        "final_offsets=[]\n",
        "for x in predicted_spans:\n",
        "  lis=[]\n",
        "  for i in range(len(x)):\n",
        "    if x[i]==1:\n",
        "      lis.append(i)\n",
        "  final_offsets.append(lis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EeCbt5cwvFC"
      },
      "source": [
        "save_path = 'drive/MyDrive/Ensemble/predicted.csv'\n",
        "data = {'spans':final_offsets,'text':use_text}\n",
        " \n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.to_csv(save_path,index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N1BHZltxEmv",
        "outputId": "a44a8e7f-6b0e-4629-d030-53090aacad9f"
      },
      "source": [
        "test_path = 'drive/MyDrive/Ensemble/testfinal.csv' # enter the path to csv where the test spans are available \n",
        "pred_path='drive/MyDrive/Ensemble/predicted.csv'  # enter the path where is predicted.csv\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from ast import literal_eval\n",
        "\n",
        "test = pd.read_csv(test_path)\n",
        "test.spans = test.spans.apply(literal_eval)\n",
        "\n",
        "prediction = pd.read_csv(pred_path)\n",
        "prediction.spans = prediction.spans.apply(literal_eval)\n",
        "\n",
        "\n",
        "spans = test.spans.tolist() #list of lists\n",
        "predicted = prediction.spans.tolist() #list of lists \n",
        "\n",
        "\n",
        "#set1: predicted, set2: ground\n",
        "\n",
        "def precision_(set1, set2):\n",
        "    return len(set1.intersection(set2))/len(set1)\n",
        "\n",
        "def recall_(set1, set2):\n",
        "    return len(set1.intersection(set2))/len(set2)\n",
        "\n",
        "def f1_(set1, set2):\n",
        "    if (len(set1) == 0 and len(set2) == 0):\n",
        "        return 1\n",
        "    elif (len(set1) == 0 or len(set2) == 0):\n",
        "        return 0\n",
        "    else:\n",
        "        p = precision_(set1, set2)\n",
        "        r = recall_(set1, set2)\n",
        "        if (p+r) == 0:\n",
        "            return 0\n",
        "        return  2*p*r/(p+r) \n",
        "\n",
        "f1s = []\n",
        "for i in range(len(spans)):\n",
        "    f1s.append(f1_(set(predicted[i]), set(spans[i])))\n",
        "\n",
        "print(sum(f1s)/len(f1s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5798660102046644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TEp3tTEzX9e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
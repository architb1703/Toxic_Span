{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSL_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Toxic_Span/blob/archit/SSL_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-AvI14dqs8J",
        "outputId": "648070a8-67aa-4609-d44f-eacfb97b3a64"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/ad/d1c685967945a04f8596128b15a1ab56c51488f53312e953341af6ff22d1/contractions-0.0.43-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 16.9MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 28.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81696 sha256=2625ef26a9262e3c27015d0657f011640a7bd1e4254fad84d800ca36336d670b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.43 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t08crSDk-Yvp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5hhMEJcvkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17e759f-5f4f-486d-b29f-24904469ad33"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EgIo60L-d6f"
      },
      "source": [
        "path = '/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/civil.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G8eVvod-r5U"
      },
      "source": [
        "data = pd.read_csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frWaytv8qyhz"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_ocBsUppG2D"
      },
      "source": [
        "toxic_data = data[data['target']>=0.8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uUodWlasV9f"
      },
      "source": [
        "toxic_data = toxic_data['comment_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdFJRu_7pYVK",
        "outputId": "dcd4aa9e-dd09-4304-cd9d-2e468a78cc55"
      },
      "source": [
        "toxic_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4                       haha you guys are a bunch of losers.\n",
              "31         Yet call out all Muslims for the acts of a few...\n",
              "34         This bitch is nuts. Who would read a book by a...\n",
              "289                                         You're an idiot.\n",
              "306        Who cares!? Stark trek and Star Wars fans are ...\n",
              "                                 ...                        \n",
              "1804600    Wow. What entitled, arrogant , immature, loser...\n",
              "1804712                                          Human scum.\n",
              "1804713    This dirtbag is no Marine; he's a sadistic ter...\n",
              "1804753    I saved where I could because my children matt...\n",
              "1804825    Who is the jerk in the last row between the C ...\n",
              "Name: comment_text, Length: 30831, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt2Hu4Tisjal"
      },
      "source": [
        "toxic_data.to_csv('/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/toxic_civil.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrtWuMEnaDb"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/toxic_civil.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4169Y2endqm"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer as twt\n",
        "\n",
        "for i,string in enumerate(data['comment_text']):\n",
        "  data['comment_text'][i] = twt().tokenize(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJd3VCeJqCWp"
      },
      "source": [
        "#################### cleaning functions ####################\n",
        "# emoji removing function\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "#convert accented text\n",
        "import unicodedata\n",
        "def convert_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "#expand contraction\n",
        "import contractions\n",
        "import re \n",
        "\n",
        "# function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    return(contractions.fix(text))\n",
        "# function to remove digits\n",
        "def remove_nums(text):\n",
        "    return re.sub('[0-9]+', '', text)\n",
        "\n",
        "# remove special chars\n",
        "def remove_spe_chars(text):\n",
        "    return re.sub('\\.+', '', text)\n",
        "#################### cleaning functions ####################    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdmFH9d9qvLe"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer as twt\n",
        "\n",
        "for i,string in enumerate(data['comment_text']):\n",
        "  for j,token in enumerate(data['comment_text'][i]):\n",
        "    token = expand_contractions(token) #expand contraction\n",
        "    token = remove_nums(token)#remove digits\n",
        "    token = convert_accented_chars(token)#convert accented\n",
        "    token = deEmojify(token)#remove emoji\n",
        "    token = remove_spe_chars(token)#combine all multiple full stops occurring together\n",
        "    data['comment_text'][i][j] = token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEjb3NJyrvLM"
      },
      "source": [
        "for i,string in enumerate(data['comment_text']):\n",
        "  data['comment_text'][i] = [x for x in data['comment_text'][i] if x!='']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPvCevYTz0Cu"
      },
      "source": [
        "data.to_csv('/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/train_civil.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVv4Qz6wngN9"
      },
      "source": [
        "#Creating labels for semi-supervised learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "Da-DweRTqt2U",
        "outputId": "f4f0e1aa-b7b2-4d89-dccc-3f5578bd4cb7"
      },
      "source": [
        "!pip install transformers==2.6.0\n",
        "!pip install seqeval\n",
        "!pip install urllib3 --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.6.0 in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.1.94)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (0.0.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.16.34)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 10.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.17.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.34 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.19.34)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.34->boto3->transformers==2.6.0) (2.8.1)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.26.2\n",
            "    Uninstalling urllib3-1.26.2:\n",
            "      Successfully uninstalled urllib3-1.26.2\n",
            "Successfully installed urllib3-1.25.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Collecting urllib3\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.25.11\n",
            "    Uninstalling urllib3-1.25.11:\n",
            "      Successfully uninstalled urllib3-1.25.11\n",
            "Successfully installed urllib3-1.26.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjaBCDCeqWSE"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, AdamW\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m8v9sfzqYvC"
      },
      "source": [
        "model = torch.load('/content/drive/MyDrive/semi_sup-5.pt')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-UF92A7rClr"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/train_civil.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkbz92jRrsfE"
      },
      "source": [
        "#Class to take in tokenizer and model along with input data and calculate the span average f1 score\n",
        "class Eval():\n",
        "  def __init__(self, model, tokenizer, X, maxlen=500):\n",
        "    self.model = model\n",
        "    self.model.cuda()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.maxlen = maxlen\n",
        "    self.X = X\n",
        "    self.prepare_data()\n",
        "\n",
        "    self.X = torch.tensor(self.X)\n",
        "    self.attention_mask = torch.tensor(self.attention_mask)\n",
        "\n",
        "    data = TensorDataset(self.X, self.attention_mask)\n",
        "    self.dataloader = DataLoader(data, batch_size=16, shuffle=False)\n",
        "  \n",
        "  def tokenize_data(self, x):\n",
        "    x = [a.lstrip(\"'\").rstrip(\"'\") for a in x.rstrip(\"]\").lstrip(\"[\").split(', ')]\n",
        "    sentence = []\n",
        "    for i in range(len(x)):\n",
        "      word = x[i]\n",
        "      tokenized_word = self.tokenizer.tokenize(word)\n",
        "      sentence.extend(tokenized_word)\n",
        "    return(sentence)\n",
        "\n",
        "  def get_attention_mask(self, x):\n",
        "    return([[(i!=0) for i in text] for text in x])\n",
        "\n",
        "  def prepare_data(self):\n",
        "    for i in range(len(self.X)):\n",
        "      self.X[i] = self.tokenize_data(self.X[i])\n",
        "    self.X = pad_sequences([tokenizer.encode(text) for text in self.X], maxlen = self.maxlen, dtype='long', value=0.0, truncating='post', padding = 'post')\n",
        "    self.attention_mask = self.get_attention_mask(self.X)\n",
        "  \n",
        "  def evaluate(self, flag):\n",
        "    predictions = []\n",
        "    X = []\n",
        "    for batch in tqdm(self.dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_id, b_input_mask = batch\n",
        "      self.model.eval()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        outputs = self.model(b_input_id, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = outputs[0].detach().cpu().numpy()\n",
        "      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "      X.extend(b_input_id)\n",
        "    self.pred_tags = [[p_i for p_i, l_i in zip(p, l) if l_i != 0][1:-1] for p, l in zip(predictions, X)]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvqh-rjeGncw"
      },
      "source": [
        "X = data['comment_text'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs9RPqQQHdxJ"
      },
      "source": [
        "pred = Eval(model, tokenizer, X[:10000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5oSsyDlKq9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9dea3eb-e219-417e-f22a-0134b6b0284a"
      },
      "source": [
        "pred.evaluate(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:37<00:00,  1.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A4jznrPkhLC"
      },
      "source": [
        "data['comment_text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0-wklLIUyxW"
      },
      "source": [
        "d = []\n",
        "\n",
        "for i in range(len(pred.pred_tags)):\n",
        "  d.append([data['comment_text'][i], [0]+pred.pred_tags[i]+[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBtLjATxrQEV"
      },
      "source": [
        "d = pd.DataFrame(d, columns=['token_final', 'target_final'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xLpvEB9rk5d"
      },
      "source": [
        "d.to_pickle('/content/drive/MyDrive/ToxicSpan_CS669V/BERT_Preprocess/civil_train_new_7.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}